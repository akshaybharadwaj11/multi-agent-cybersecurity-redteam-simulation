<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Multi-Agent Cybersecurity Red Team Simulation - Technical Report</title>
    <style>
        @page {
            size: A4;
            margin: 0.75in;
        }
        
        body {
            font-family: 'Georgia', 'Times New Roman', serif;
            line-height: 1.6;
            color: #333;
            max-width: 100%;
            margin: 0 auto;
            padding: 0;
            background: white;
        }
        
        .page {
            page-break-after: always;
            min-height: 9.7in;
            padding: 0.75in;
            box-sizing: border-box;
        }
        
        .page:last-child {
            page-break-after: auto;
        }
        
        h1 {
            font-size: 18pt;
            color: #1a1a1a;
            border-bottom: 3px solid #2c3e50;
            padding-bottom: 8px;
            margin-top: 0;
            margin-bottom: 12px;
        }
        
        h2 {
            font-size: 14pt;
            color: #2c3e50;
            border-bottom: 2px solid #3498db;
            padding-bottom: 6px;
            margin-top: 20px;
            margin-bottom: 10px;
        }
        
        h3 {
            font-size: 12pt;
            color: #34495e;
            margin-top: 15px;
            margin-bottom: 8px;
        }
        
        h4 {
            font-size: 11pt;
            color: #555;
            margin-top: 12px;
            margin-bottom: 6px;
        }
        
        p {
            text-align: justify;
            margin-bottom: 10px;
            font-size: 10pt;
        }
        
        ul, ol {
            margin-bottom: 10px;
            padding-left: 25px;
        }
        
        li {
            margin-bottom: 6px;
            font-size: 10pt;
        }
        
        .architecture-diagram {
            background: #f8f9fa;
            border: 1px solid #dee2e6;
            border-radius: 5px;
            padding: 4px;
            margin: 8px 0;
            font-family: 'Courier New', monospace;
            font-size: 5.5pt;
            line-height: 1.0;
            white-space: pre;
            overflow-x: auto;
            page-break-inside: avoid;
        }
        
        .code-block {
            background: #f4f4f4;
            border-left: 4px solid #3498db;
            padding: 12px;
            margin: 12px 0;
            font-family: 'Courier New', monospace;
            font-size: 8pt;
            overflow-x: auto;
            white-space: pre;
            line-height: 1.4;
        }
        
        .metrics-table {
            width: 100%;
            border-collapse: collapse;
            margin: 12px 0;
            font-size: 9pt;
            page-break-inside: avoid;
        }
        
        .metrics-table th,
        .metrics-table td {
            border: 1px solid #ddd;
            padding: 6px;
            text-align: left;
        }
        
        .metrics-table th {
            background-color: #3498db;
            color: white;
            font-weight: bold;
        }
        
        .metrics-table tr:nth-child(even) {
            background-color: #f2f2f2;
        }
        
        .highlight-box {
            background: #e8f4f8;
            border-left: 4px solid #3498db;
            padding: 12px;
            margin: 12px 0;
            font-size: 10pt;
        }
        
        .footer {
            position: fixed;
            bottom: 0.5in;
            width: 100%;
            text-align: center;
            font-size: 9pt;
            color: #666;
        }
        
        .page-number {
            position: fixed;
            bottom: 0.5in;
            right: 1in;
            font-size: 9pt;
            color: #666;
        }
        
        .abstract {
            background: #fff9e6;
            border: 1px solid #ffd700;
            padding: 12px;
            margin: 12px 0;
            border-radius: 5px;
        }
        
        .abstract p {
            font-size: 10pt;
        }
        
        .section {
            margin-bottom: 30px;
        }
        
        table {
            width: 100%;
            border-collapse: collapse;
            margin: 12px 0;
            font-size: 9pt;
            page-break-inside: avoid;
        }
        
        table th, table td {
            border: 1px solid #ddd;
            padding: 6px;
            text-align: left;
        }
        
        table th {
            background-color: #2c3e50;
            color: white;
        }
        
        .tech-stack {
            display: grid;
            grid-template-columns: 1fr 1fr;
            gap: 10px;
            margin: 12px 0;
        }
        
        .tech-item {
            background: #f8f9fa;
            padding: 8px;
            border-radius: 5px;
            border-left: 3px solid #3498db;
            font-size: 9pt;
        }
        
        .tech-item strong {
            color: #2c3e50;
        }
        
        .chart-container {
            background: white;
            border: 1px solid #ddd;
            border-radius: 5px;
            padding: 10px;
            margin: 12px 0;
            text-align: center;
        }
        
        .chart-title {
            font-size: 11pt;
            font-weight: bold;
            color: #2c3e50;
            margin-bottom: 6px;
        }
        
        .chart-subtitle {
            font-size: 9pt;
            color: #666;
            margin-bottom: 10px;
        }
        
        svg {
            max-width: 100%;
            height: auto;
        }
        
        .chart-legend {
            margin-top: 8px;
            font-size: 8pt;
            color: #666;
        }
        
        .legend-item {
            display: inline-block;
            margin: 5px 15px;
        }
        
        .legend-color {
            display: inline-block;
            width: 15px;
            height: 15px;
            margin-right: 5px;
            vertical-align: middle;
        }
    </style>
</head>
<body>
    <!-- Title Page -->
    <div class="page">
        <div style="text-align: center; padding-top: 2.2in; display: flex; flex-direction: column; height: 7.5in; justify-content: flex-start; gap: 0.3in;">
            <div>
                <h1 style="border: none; font-size: 24pt; margin-bottom: 10px; line-height: 1.2;">
                Multi-Agent Cybersecurity<br>Red Team Simulation
            </h1>
                <h2 style="border: none; font-size: 14pt; color: #555; margin-bottom: 8px;">
                Technical Report
            </h2>
                <p style="font-size: 12pt; color: #444; margin: 0; font-weight: 500; margin-top: 4px; text-align: center;">December 2025</p>
            </div>
            
            <div style="font-size: 10pt; color: #666; margin-top: 0.3in;">
                <p style="margin-bottom: 4px;"><strong>Generative AI Project</strong></p>
                <p style="margin-bottom: 12px;">Advanced Cybersecurity Simulation Platform</p>
                
                <div style="font-size: 11pt; color: #333; margin-top: 0.2in;">
                    <p style="margin-bottom: 6px;"><strong>Authors:</strong></p>
                    <p style="margin-bottom: 3px;">Suhas Reddy</p>
                    <p style="margin-bottom: 3px;">Akshay Bharadwaj</p>
                    <p style="margin-bottom: 3px;">Vemana Anilkumar</p>
                </div>
            </div>
        </div>
    </div>

    <!-- Table of Contents -->
    <div class="page">
        <h1>Table of Contents</h1>
        <ol style="font-size: 12pt; line-height: 2;">
            <li><a href="#abstract">Abstract</a></li>
            <li><a href="#introduction">Introduction</a></li>
            <li><a href="#architecture">System Architecture</a></li>
            <li><a href="#implementation">Implementation Details</a></li>
            <li><a href="#performance">Performance Metrics</a></li>
            <li><a href="#challenges">Challenges and Solutions</a></li>
            <li><a href="#future">Future Improvements</a></li>
            <li><a href="#ethics">Ethical Considerations</a></li>
            <li><a href="#conclusion">Conclusion</a></li>
            <li><a href="#examples">Sample Outputs and Examples</a></li>
            <li><a href="#comparison">Comparison and Baseline</a></li>
            <li><a href="#appendix">Appendix</a></li>
        </ol>
    </div>

    <!-- Abstract -->
    <div class="page" id="abstract">
        <h1>Abstract</h1>
        <div class="abstract">
            <p>This report presents a comprehensive technical analysis of a multi-agent cybersecurity simulation platform that combines generative AI technologies including Retrieval-Augmented Generation (RAG), Reinforcement Learning (RL), and Large Language Models (LLMs) to create realistic red team vs. blue team security scenarios. The system employs CrewAI for multi-agent orchestration, ChromaDB for vector storage, and a contextual bandit RL algorithm to learn optimal defense strategies.</p>
            
            <p>The platform simulates sophisticated cyberattack scenarios using the MITRE ATT&CK framework, generates synthetic telemetry data, detects incidents through LLM-powered analysis, retrieves relevant security knowledge through RAG, and makes adaptive remediation decisions through reinforcement learning. Training results demonstrate a 90.0% success rate in attack containment after 10 episodes, with a 90.0% detection rate and an average reward of 1.13, showing strong initial learning performance.</p>
            
            <p>This project demonstrates the practical application of multiple generative AI techniques in a unified system, showcasing how RAG, fine-tuning concepts, and RL can work together to solve complex real-world cybersecurity challenges.</p>
        </div>
    </div>

    <!-- Introduction -->
    <div class="page" id="introduction">
        <h1>1. Introduction</h1>
        
        <h2>1.1 Project Overview</h2>
        <p>The Multi-Agent Cybersecurity Red Team Simulation is an advanced generative AI system designed to simulate realistic cyberattack scenarios and train adaptive defense mechanisms. The system addresses the critical need for continuous security training and automated incident response in modern cybersecurity operations.</p>
        
        <h2>1.2 Objectives</h2>
        <ul>
            <li><strong>Realistic Attack Simulation:</strong> Generate multi-stage attack scenarios based on the MITRE ATT&CK framework</li>
            <li><strong>Intelligent Detection:</strong> Use LLM-powered agents to detect and analyze security incidents from telemetry data</li>
            <li><strong>Knowledge Retrieval:</strong> Implement RAG to retrieve relevant security runbooks, threat intelligence, and historical incidents</li>
            <li><strong>Adaptive Defense:</strong> Train reinforcement learning agents to learn optimal remediation strategies through experience</li>
            <li><strong>Explainable AI:</strong> Provide full audit trails of decisions, evidence, and justifications</li>
        </ul>
        
        <h2>1.3 Generative AI Components Implemented</h2>
        <div class="tech-stack">
            <div class="tech-item">
                <strong>Retrieval-Augmented Generation (RAG)</strong><br>
                Vector-based knowledge retrieval system using ChromaDB and sentence transformers
            </div>
            <div class="tech-item">
                <strong>Reinforcement Learning</strong><br>
                Contextual bandit algorithm for adaptive decision-making
            </div>
            <div class="tech-item">
                <strong>Prompt Engineering</strong><br>
                Systematic prompting strategies for multi-agent coordination
            </div>
            <div class="tech-item">
                <strong>Synthetic Data Generation</strong><br>
                Realistic telemetry and log generation for training
            </div>
        </div>
        
        <h2>1.4 Technology Stack</h2>
        <table style="page-break-inside: avoid;">
            <tr>
                <th>Category</th>
                <th>Technology</th>
                <th>Purpose</th>
            </tr>
            <tr>
                <td>Multi-Agent Framework</td>
                <td>CrewAI</td>
                <td>Agent orchestration and coordination</td>
            </tr>
            <tr>
                <td>LLM</td>
                <td>Groq (Llama 3.1 70B) / OpenAI GPT</td>
                <td>Natural language reasoning and generation</td>
            </tr>
            <tr>
                <td>Vector Database</td>
                <td>ChromaDB</td>
                <td>RAG knowledge base storage</td>
            </tr>
            <tr>
                <td>Embeddings</td>
                <td>Sentence Transformers / OpenAI</td>
                <td>Semantic search and retrieval</td
            </tr>
            <tr>
                <td>RL Framework</td>
                <td>Custom Contextual Bandit (PyTorch)</td>
                <td>Adaptive decision learning</td>
            </tr>
            <tr>
                <td>Data Models</td>
                <td>Pydantic</td>
                <td>Type-safe data validation</td>
            </tr>
            <tr>
                <td>Web Framework</td>
                <td>FastAPI + React</td>
                <td>API and dashboard interface</td>
            </tr>
        </table>
    </div>

    <!-- System Architecture -->
    <div class="page" id="architecture">
        <h1>2. System Architecture</h1>
        
        <h2>2.1 High-Level Architecture</h2>
        
        <!-- Visualization 5: System Architecture Flow Diagram -->
        <div class="chart-container">
            <div class="chart-title">System Architecture and Data Flow</div>
            <div class="chart-subtitle">Multi-agent workflow from attack generation to remediation</div>
            <svg width="900" height="700" viewBox="0 0 900 700" xmlns="http://www.w3.org/2000/svg" style="max-width: 100%; background: #ffffff;">
                <defs>
                    <!-- Arrow markers - using only 2 colors: blue and gray -->
                    <marker id="arrow-primary" markerWidth="10" markerHeight="10" refX="9" refY="3" orient="auto" markerUnits="strokeWidth">
                        <path d="M 0,0 L 10,3 L 0,6 Z" fill="#2563eb" stroke="none"/>
                    </marker>
                    <marker id="arrow-secondary" markerWidth="10" markerHeight="10" refX="9" refY="3" orient="auto" markerUnits="strokeWidth">
                        <path d="M 0,0 L 10,3 L 0,6 Z" fill="#64748b" stroke="none"/>
                    </marker>
                </defs>
                
                <!-- Layer 1: Orchestrator (Top Center) -->
                <rect x="350" y="30" width="200" height="70" rx="6" fill="#2563eb" stroke="none"/>
                <text x="450" y="60" font-family="'Segoe UI', Arial, sans-serif" font-size="18" fill="white" text-anchor="middle" font-weight="600">Orchestrator</text>
                <text x="450" y="80" font-family="'Segoe UI', Arial, sans-serif" font-size="11" fill="rgba(255,255,255,0.9)" text-anchor="middle">Workflow Coordinator</text>
                
                <!-- Layer 2: Primary Agents (Row 1) - evenly spaced -->
                <!-- Red Team Agent -->
                <rect x="50" y="140" width="160" height="80" rx="6" fill="#f8fafc" stroke="#64748b" stroke-width="2"/>
                <text x="130" y="165" font-family="'Segoe UI', Arial, sans-serif" font-size="14" fill="#1e293b" text-anchor="middle" font-weight="600">Red Team Agent</text>
                <text x="130" y="185" font-family="'Segoe UI', Arial, sans-serif" font-size="10" fill="#64748b" text-anchor="middle">Attack Generation</text>
                <text x="130" y="200" font-family="'Segoe UI', Arial, sans-serif" font-size="9" fill="#94a3b8" text-anchor="middle">MITRE ATT&CK</text>
                
                <!-- Telemetry Generator -->
                <rect x="250" y="140" width="160" height="80" rx="6" fill="#f8fafc" stroke="#64748b" stroke-width="2"/>
                <text x="330" y="165" font-family="'Segoe UI', Arial, sans-serif" font-size="14" fill="#1e293b" text-anchor="middle" font-weight="600">Telemetry Generator</text>
                <text x="330" y="185" font-family="'Segoe UI', Arial, sans-serif" font-size="10" fill="#64748b" text-anchor="middle">System Logs</text>
                <text x="330" y="200" font-family="'Segoe UI', Arial, sans-serif" font-size="9" fill="#94a3b8" text-anchor="middle">Auth/Network/Process</text>
                
                <!-- Detection Agent -->
                <rect x="450" y="140" width="160" height="80" rx="6" fill="#f8fafc" stroke="#64748b" stroke-width="2"/>
                <text x="530" y="165" font-family="'Segoe UI', Arial, sans-serif" font-size="14" fill="#1e293b" text-anchor="middle" font-weight="600">Detection Agent</text>
                <text x="530" y="185" font-family="'Segoe UI', Arial, sans-serif" font-size="10" fill="#64748b" text-anchor="middle">Incident Analysis</text>
                <text x="530" y="200" font-family="'Segoe UI', Arial, sans-serif" font-size="9" fill="#94a3b8" text-anchor="middle">LLM Reasoning</text>
                
                <!-- RAG Agent -->
                <rect x="650" y="140" width="160" height="80" rx="6" fill="#f8fafc" stroke="#64748b" stroke-width="2"/>
                <text x="730" y="165" font-family="'Segoe UI', Arial, sans-serif" font-size="14" fill="#1e293b" text-anchor="middle" font-weight="600">RAG Agent</text>
                <text x="730" y="185" font-family="'Segoe UI', Arial, sans-serif" font-size="10" fill="#64748b" text-anchor="middle">Context Retrieval</text>
                <text x="730" y="200" font-family="'Segoe UI', Arial, sans-serif" font-size="9" fill="#94a3b8" text-anchor="middle">Vector Search</text>
                
                <!-- Layer 3: Decision & Action (Row 2) - centered -->
                <!-- Remediation Agent -->
                <rect x="200" y="270" width="180" height="80" rx="6" fill="#f8fafc" stroke="#64748b" stroke-width="2"/>
                <text x="290" y="295" font-family="'Segoe UI', Arial, sans-serif" font-size="14" fill="#1e293b" text-anchor="middle" font-weight="600">Remediation Agent</text>
                <text x="290" y="315" font-family="'Segoe UI', Arial, sans-serif" font-size="10" fill="#64748b" text-anchor="middle">Action Planning</text>
                <text x="290" y="330" font-family="'Segoe UI', Arial, sans-serif" font-size="9" fill="#94a3b8" text-anchor="middle">Risk Assessment</text>
                
                <!-- RL Agent -->
                <rect x="420" y="270" width="180" height="80" rx="6" fill="#2563eb" stroke="none"/>
                <text x="510" y="295" font-family="'Segoe UI', Arial, sans-serif" font-size="14" fill="white" text-anchor="middle" font-weight="600">RL Policy Agent</text>
                <text x="510" y="315" font-family="'Segoe UI', Arial, sans-serif" font-size="10" fill="rgba(255,255,255,0.9)" text-anchor="middle">Action Selection</text>
                <text x="510" y="330" font-family="'Segoe UI', Arial, sans-serif" font-size="9" fill="rgba(255,255,255,0.8)" text-anchor="middle">Q-Learning</text>
                
                <!-- Reward Calculator -->
                <rect x="640" y="270" width="180" height="80" rx="6" fill="#f8fafc" stroke="#64748b" stroke-width="2"/>
                <text x="730" y="295" font-family="'Segoe UI', Arial, sans-serif" font-size="14" fill="#1e293b" text-anchor="middle" font-weight="600">Reward Calculator</text>
                <text x="730" y="315" font-family="'Segoe UI', Arial, sans-serif" font-size="10" fill="#64748b" text-anchor="middle">Outcome Evaluation</text>
                <text x="730" y="330" font-family="'Segoe UI', Arial, sans-serif" font-size="9" fill="#94a3b8" text-anchor="middle">Feedback Loop</text>
                
                <!-- Layer 4: Storage & Learning (Row 3) - evenly spaced -->
                <!-- Knowledge Base -->
                <rect x="50" y="400" width="200" height="100" rx="6" fill="#f8fafc" stroke="#64748b" stroke-width="2"/>
                <text x="150" y="425" font-family="'Segoe UI', Arial, sans-serif" font-size="13" fill="#1e293b" text-anchor="middle" font-weight="600">Knowledge Base</text>
                <text x="150" y="445" font-family="'Segoe UI', Arial, sans-serif" font-size="10" fill="#64748b" text-anchor="middle">• Runbooks</text>
                <text x="150" y="465" font-family="'Segoe UI', Arial, sans-serif" font-size="10" fill="#64748b" text-anchor="middle">• MITRE ATT&CK</text>
                <text x="150" y="485" font-family="'Segoe UI', Arial, sans-serif" font-size="10" fill="#64748b" text-anchor="middle">• Threat Intel</text>
                
                <!-- Vector Store -->
                <rect x="300" y="400" width="200" height="100" rx="6" fill="#f8fafc" stroke="#64748b" stroke-width="2"/>
                <text x="400" y="425" font-family="'Segoe UI', Arial, sans-serif" font-size="13" fill="#1e293b" text-anchor="middle" font-weight="600">Vector Store</text>
                <text x="400" y="445" font-family="'Segoe UI', Arial, sans-serif" font-size="10" fill="#64748b" text-anchor="middle">• ChromaDB</text>
                <text x="400" y="465" font-family="'Segoe UI', Arial, sans-serif" font-size="10" fill="#64748b" text-anchor="middle">• Embeddings</text>
                <text x="400" y="485" font-family="'Segoe UI', Arial, sans-serif" font-size="10" fill="#64748b" text-anchor="middle">• Semantic Search</text>
                
                <!-- Q-Table -->
                <rect x="550" y="400" width="200" height="100" rx="6" fill="#f8fafc" stroke="#64748b" stroke-width="2"/>
                <text x="650" y="425" font-family="'Segoe UI', Arial, sans-serif" font-size="13" fill="#1e293b" text-anchor="middle" font-weight="600">Q-Table</text>
                <text x="650" y="445" font-family="'Segoe UI', Arial, sans-serif" font-size="10" fill="#64748b" text-anchor="middle">• State-Action Pairs</text>
                <text x="650" y="465" font-family="'Segoe UI', Arial, sans-serif" font-size="10" fill="#64748b" text-anchor="middle">• Q-Values</text>
                <text x="650" y="485" font-family="'Segoe UI', Arial, sans-serif" font-size="10" fill="#64748b" text-anchor="middle">• RL Memory</text>
                
                <!-- Primary Flow Arrows (Blue) - Clean, non-overlapping paths -->
                <!-- Orchestrator to Row 1 -->
                <line x1="400" y1="100" x2="130" y2="140" stroke="#2563eb" stroke-width="2.5" marker-end="url(#arrow-primary)"/>
                <line x1="420" y1="100" x2="330" y2="140" stroke="#2563eb" stroke-width="2.5" marker-end="url(#arrow-primary)"/>
                <line x1="440" y1="100" x2="530" y2="140" stroke="#2563eb" stroke-width="2.5" marker-end="url(#arrow-primary)"/>
                <line x1="460" y1="100" x2="730" y2="140" stroke="#2563eb" stroke-width="2.5" marker-end="url(#arrow-primary)"/>
                
                <!-- Row 1 Horizontal Flow: Red Team → Telemetry → Detection → RAG -->
                <line x1="210" y1="180" x2="250" y2="180" stroke="#2563eb" stroke-width="2.5" marker-end="url(#arrow-primary)"/>
                <line x1="410" y1="180" x2="450" y2="180" stroke="#2563eb" stroke-width="2.5" marker-end="url(#arrow-primary)"/>
                <line x1="610" y1="180" x2="650" y2="180" stroke="#2563eb" stroke-width="2.5" marker-end="url(#arrow-primary)"/>
                
                <!-- Row 1 to Row 2: Detection → Remediation, RL, Reward (clean vertical paths) -->
                <line x1="530" y1="220" x2="290" y2="270" stroke="#2563eb" stroke-width="2.5" marker-end="url(#arrow-primary)"/>
                <line x1="550" y1="220" x2="510" y2="270" stroke="#2563eb" stroke-width="2.5" marker-end="url(#arrow-primary)"/>
                <line x1="570" y1="220" x2="730" y2="270" stroke="#2563eb" stroke-width="2.5" marker-end="url(#arrow-primary)"/>
                
                <!-- Row 2: RL → Remediation (horizontal) -->
                <line x1="420" y1="310" x2="380" y2="310" stroke="#2563eb" stroke-width="2.5" marker-end="url(#arrow-primary)"/>
                
                <!-- Row 2 to Row 3: Reward → RL (Feedback - clean curved path) -->
                <path d="M 730 350 Q 620 380 510 350" stroke="#2563eb" stroke-width="2.5" fill="none" marker-end="url(#arrow-primary)" stroke-dasharray="5,3"/>
                
                <!-- Secondary Flow Arrows (Gray) - Data access paths, no overlaps -->
                <!-- RAG → Vector Store (direct) -->
                <line x1="730" y1="220" x2="400" y2="400" stroke="#64748b" stroke-width="2" marker-end="url(#arrow-secondary)" stroke-dasharray="4,3" opacity="0.6"/>
                
                <!-- Vector Store → Knowledge Base (horizontal) -->
                <line x1="300" y1="450" x2="250" y2="450" stroke="#64748b" stroke-width="2" marker-end="url(#arrow-secondary)" stroke-dasharray="4,3" opacity="0.6"/>
                
                <!-- Knowledge Base → Remediation (upward) -->
                <path d="M 150 400 Q 100 350 150 350 Q 200 350 290 350" stroke="#64748b" stroke-width="2" marker-end="url(#arrow-secondary)" stroke-dasharray="4,3" opacity="0.6" fill="none"/>
                
                <!-- RL Agent ↔ Q-Table (bidirectional, clean paths) -->
                <line x1="510" y1="350" x2="650" y2="400" stroke="#64748b" stroke-width="2" marker-end="url(#arrow-secondary)" stroke-dasharray="4,3" opacity="0.6"/>
                <path d="M 650 450 Q 600 420 580 400 Q 560 380 510 350" stroke="#64748b" stroke-width="2" marker-end="url(#arrow-secondary)" stroke-dasharray="4,3" opacity="0.6" fill="none"/>
                
                <!-- Flow Labels - minimal, clean -->
                <text x="230" y="175" font-family="'Segoe UI', Arial, sans-serif" font-size="9" fill="#2563eb" text-anchor="middle" font-weight="500">Attack</text>
                <text x="430" y="175" font-family="'Segoe UI', Arial, sans-serif" font-size="9" fill="#2563eb" text-anchor="middle" font-weight="500">Logs</text>
                <text x="630" y="175" font-family="'Segoe UI', Arial, sans-serif" font-size="9" fill="#2563eb" text-anchor="middle" font-weight="500">Context</text>
                <text x="400" y="305" font-family="'Segoe UI', Arial, sans-serif" font-size="9" fill="#2563eb" text-anchor="middle" font-weight="500">Actions</text>
                <text x="620" y="360" font-family="'Segoe UI', Arial, sans-serif" font-size="9" fill="#2563eb" text-anchor="middle" font-weight="500">Feedback</text>
            </svg>
        </div>
        
        <div class="architecture-diagram" style="font-size: 5.5pt; line-height: 1.2; padding: 8px;">
┌─────────────────────────────────────────────────────────────────────┐
│              Cyber Defense Orchestrator                             │
│         (Multi-Agent Coordination & Workflow Management)            │
└─────────────────────────────────────────────────────────────────────┘
                        │                    │
                        ▼                    ▼
        ┌──────────────────────┐    ┌──────────────────────┐
        │   Red Team Agent     │    │  Detection Agent     │
        │   (Attack Gen)       │───►│  (Incident Anal)     │
        │   - MITRE ATT&CK     │    │  - LLM analysis      │
        └──────────────────────┘    └──────────────────────┘
                │                            │
                │                            ▼
                │            ┌──────────────────────┐
                │            │ Telemetry Generator  │
                │            │ - System/Auth/Net    │
                │            │ - Process logs       │
                │            └──────────────────────┘
                │                            │
                │                            ▼
                │            ┌──────────────────────┐
                │            │     RAG Agent        │
                │            │ (Context Retrieval)  │
                │            │ - Vector/Runbook     │
                │            │ - Threat intel       │
                │            └──────────────────────┘
                │                            │
                │                            ▼
                │            ┌──────────────────────┐
                │            │ Remediation Agent    │
                │            │ (Action Planning)    │
                │            │ - Recommendations    │
                │            └──────────────────────┘
                │                            │
                │                            ▼
                │            ┌──────────────────────┐
                │            │  RL Policy Agent     │
                │            │ (Action Selection)   │
                │            │ - Contextual bandit  │
                │            │ - Q-learning         │
                │            └──────────────────────┘
                │                            │
                │                            ▼
                │            ┌──────────────────────┐
                │            │ Reward Calculator    │
                │            │ (Feedback Loop)      │
                │            │ - Outcome/Reward     │
                │            └──────────────────────┘
                │                            │
                └────────────────────────────┘
                        (Feedback to RL Agent)
        </div>
        
        <h2>2.2 Component Interactions</h2>
        <p>The system follows an 8-step workflow for each simulation episode:</p>
        <ol>
            <li><strong>Attack Generation:</strong> Red Team Agent generates a multi-stage attack scenario using MITRE ATT&CK techniques</li>
            <li><strong>Telemetry Generation:</strong> Synthetic logs are created based on the attack scenario, including system, authentication, network, and process logs</li>
            <li><strong>Incident Detection:</strong> Detection Agent analyzes telemetry using LLM reasoning to identify anomalies and generate incident reports</li>
            <li><strong>Context Retrieval:</strong> RAG Agent searches the knowledge base for relevant runbooks, threat intelligence, and similar past incidents</li>
            <li><strong>Remediation Planning:</strong> Remediation Agent generates multiple action options with risk assessments and justifications</li>
            <li><strong>Action Selection:</strong> RL Agent selects the optimal action based on learned Q-values and current exploration policy</li>
            <li><strong>Outcome Simulation:</strong> The system simulates the result of the selected action</li>
            <li><strong>Learning Update:</strong> Reward is calculated and RL agent updates its Q-values based on the outcome</li>
        </ol>
        
        <h2>2.3 Data Flow</h2>
        <div class="highlight-box">
            <p><strong>State Representation:</strong> The RL agent uses a 5-dimensional feature vector representing:</p>
            <ul>
                <li>Incident severity (normalized: 0.25-1.0)</li>
                <li>Attack type (normalized: 0.0-1.0)</li>
                <li>Confidence level (0.0-1.0)</li>
                <li>Number of affected assets (normalized: 0.0-1.0)</li>
                <li>Number of MITRE techniques (normalized: 0.0-1.0)</li>
            </ul>
        </div>
        
        <h2>2.4 Knowledge Base Structure</h2>
        <p>The RAG system maintains a vector store containing:</p>
        <ul>
            <li><strong>Security Runbooks:</strong> Step-by-step procedures for responding to specific attack types</li>
            <li><strong>MITRE ATT&CK Techniques:</strong> Detailed descriptions of adversary tactics and techniques</li>
            <li><strong>CVE Data:</strong> Common Vulnerabilities and Exposures information</li>
            <li><strong>Historical Incidents:</strong> Past incident reports for similarity matching</li>
            <li><strong>Threat Intelligence:</strong> IOCs, TTPs, and threat actor profiles</li>
        </ul>
    </div>

    <!-- Implementation Details -->
    <div class="page" id="implementation">
        <h1>3. Implementation Details</h1>
        
        <h2>3.1 RAG Implementation</h2>
        <h3>3.1.1 Vector Store Architecture</h3>
        <p>The RAG system uses ChromaDB as the persistent vector store with the following configuration:</p>
        <ul>
            <li><strong>Embedding Model:</strong> Sentence Transformers (all-MiniLM-L6-v2) or OpenAI text-embedding-3-small</li>
            <li><strong>Embedding Dimension:</strong> 1536 (OpenAI) or 384 (Sentence Transformers)</li>
            <li><strong>Retrieval Strategy:</strong> Top-K similarity search with metadata filtering</li>
            <li><strong>Chunking Strategy:</strong> Document-based chunking with technique-specific metadata</li>
        </ul>
        
        <h3>3.1.2 Retrieval Process</h3>
        <div class="code-block">
1. Query Construction: Build search query from incident summary + MITRE techniques
2. Embedding Generation: Convert query to vector representation
3. Similarity Search: Find top-K documents using cosine similarity
4. Metadata Filtering: Filter by document type (runbook, technique, CVE)
5. Relevance Scoring: Rank results by similarity score
6. Context Assembly: Combine retrieved documents for LLM context
        </div>
        
        <h3>3.1.3 Knowledge Base Initialization</h3>
        <p>The knowledge base is populated with:</p>
        <ul>
            <li>50+ security runbooks covering all attack types</li>
            <li>200+ MITRE ATT&CK technique descriptions</li>
            <li>Historical incident data from past simulations</li>
            <li>CVE information for vulnerability context</li>
        </ul>
        
        <h2>3.2 Reinforcement Learning Implementation</h2>
        <h3>3.2.1 Contextual Bandit Algorithm</h3>
        <p>The RL agent implements a contextual bandit with the following characteristics:</p>
        <ul>
            <li><strong>State Space:</strong> 5-dimensional continuous feature vector</li>
            <li><strong>Action Space:</strong> 8 discrete remediation actions</li>
            <li><strong>Policy:</strong> Epsilon-greedy with exponential decay</li>
            <li><strong>Learning:</strong> Q-learning updates with state-action value function</li>
        </ul>
        
        <h3>3.2.2 Q-Learning Update Rule</h3>
        <div class="code-block">
Q(s, a) ← Q(s, a) + α[r + γ * max Q(s', a') - Q(s, a)]

Where:
- α (learning_rate) = 0.08
- γ (discount_factor) = 0.95
- r = reward from outcome
- s = current state
- a = selected action
        </div>
        
        <h3>3.2.3 Exploration Strategy</h3>
        <p>The agent uses epsilon-greedy exploration with the following schedule:</p>
        <ul>
            <li><strong>Initial Epsilon:</strong> 1.0 (100% exploration)</li>
            <li><strong>Decay Rate:</strong> 0.999 per episode</li>
            <li><strong>Minimum Epsilon:</strong> 0.15 (maintains 15% exploration)</li>
        </ul>
        
        <h2>3.3 Reward Function Design</h2>
        <p>The reward function is carefully designed to balance multiple objectives:</p>
        <table>
            <tr>
                <th>Outcome</th>
                <th>Reward</th>
                <th>Rationale</th>
            </tr>
            <tr>
                <td>Successful containment</td>
                <td>+1.5</td>
                <td>Primary objective - stop the attack</td>
            </tr>
            <tr>
                <td>Failed containment</td>
                <td>-0.8</td>
                <td>Penalty for failure (reduced to encourage exploration)</td>
            </tr>
            <tr>
                <td>False positive</td>
                <td>-0.4</td>
                <td>Penalty for unnecessary action</td>
            </tr>
            <tr>
                <td>Collateral damage</td>
                <td>-0.1</td>
                <td>Small penalty for affecting legitimate services</td>
            </tr>
            <tr>
                <td>Speed bonus</td>
                <td>+0.3 (max)</td>
                <td>Bonus for fast response times (< 10 minutes)</td>
            </tr>
            <tr>
                <td>Time penalty</td>
                <td>-0.003 per minute</td>
                <td>Minimal penalty to encourage efficiency</td>
            </tr>
        </table>
        
        <h2>3.4 Multi-Agent Orchestration</h2>
        <h3>3.4.1 CrewAI Integration</h3>
        <p>Each agent is implemented as a CrewAI Agent with specialized roles:</p>
        <ul>
            <li><strong>Red Team Agent:</strong> Penetration tester role, generates attack scenarios</li>
            <li><strong>Detection Agent:</strong> Security analyst role, analyzes telemetry</li>
            <li><strong>RAG Agent:</strong> Threat intelligence analyst role, retrieves context</li>
            <li><strong>Remediation Agent:</strong> Incident responder role, plans actions</li>
        </ul>
        
        <h3>3.4.2 Agent Communication</h3>
        <p>Agents communicate through structured data models (Pydantic) ensuring type safety and validation:</p>
        <ul>
            <li>AttackScenario → TelemetryData → IncidentReport → RAGContext → RemediationPlan → RLDecision</li>
            <li>Each transition maintains full audit trail</li>
            <li>All data is serializable for logging and analysis</li>
        </ul>
        
        <h2>3.5 Prompt Engineering</h2>
        <h3>3.5.1 Systematic Prompting Strategies</h3>
        <p>Each agent uses carefully crafted prompts with:</p>
        <ul>
            <li><strong>Role Definition:</strong> Clear agent role and expertise</li>
            <li><strong>Context Window:</strong> Relevant information from previous steps</li>
            <li><strong>Output Format:</strong> Structured JSON responses via Pydantic</li>
            <li><strong>Error Handling:</strong> Fallback prompts for edge cases</li>
        </ul>
        
        <h3>3.5.2 Example: Detection Agent Prompt</h3>
        <div class="code-block">
Role: Senior Security Analyst
Goal: Detect security incidents from telemetry data
Context: [Telemetry logs with timestamps, sources, log levels]
Output: Structured IncidentReport with:
  - Severity level (low/medium/high/critical)
  - Confidence score (0.0-1.0)
  - Detected anomalies
  - MITRE ATT&CK techniques
  - Affected assets
        </div>
    </div>

    <!-- Performance Metrics -->
    <div class="page" id="performance">
        <h1>4. Performance Metrics</h1>
        
        <h2>4.1 Training Results and Performance Metrics</h2>
        <p>The system tracks comprehensive metrics across training episodes. The following metrics are now available in the RL Metrics dashboard:</p>
        
        <h3>4.1.1 Core Training Metrics</h3>
        <table class="metrics-table">
            <tr>
                <th>Metric</th>
                <th>Description</th>
                <th>Analysis</th>
            </tr>
            <tr>
                <td>Total Episodes</td>
                <td>Count of completed training episodes</td>
                <td>Tracks overall training progress</td>
            </tr>
            <tr>
                <td>Q-Value Updates</td>
                <td>Number of Q-table updates performed</td>
                <td>Indicates active learning activity</td>
            </tr>
            <tr>
                <td>States Learned</td>
                <td>Unique states in Q-table</td>
                <td>Measures state space exploration</td>
            </tr>
            <tr>
                <td>Average Q-Value</td>
                <td>Mean Q-value across all state-action pairs</td>
                <td>Indicates overall policy quality</td>
            </tr>
            <tr>
                <td>Max Q-Value</td>
                <td>Highest Q-value observed across all states</td>
                <td>Shows best-case action value</td>
            </tr>
        </table>
        
        <h3>4.1.2 Performance Metrics</h3>
        <table class="metrics-table">
            <tr>
                <th>Metric</th>
                <th>Description</th>
                <th>Analysis</th>
            </tr>
            <tr>
                <td>Success Rate</td>
                <td>Percentage of successful attack containments</td>
                <td>Primary performance indicator</td>
            </tr>
            <tr>
                <td>Success Trend</td>
                <td>Change in success rate (recent vs early episodes)</td>
                <td>Indicates learning improvement</td>
            </tr>
            <tr>
                <td>Average Reward</td>
                <td>Mean reward across all episodes</td>
                <td>Overall policy performance</td>
            </tr>
            <tr>
                <td>Max Reward</td>
                <td>Highest reward achieved in any episode</td>
                <td>Best-case performance</td>
            </tr>
            <tr>
                <td>Min Reward</td>
                <td>Lowest reward observed</td>
                <td>Worst-case performance</td>
            </tr>
            <tr>
                <td>Reward Trend</td>
                <td>Change in average reward (recent vs early)</td>
                <td>Learning trajectory indicator</td>
            </tr>
            <tr>
                <td>False Positive Rate</td>
                <td>Percentage of false positive detections</td>
                <td>Accuracy of threat assessment</td>
            </tr>
            <tr>
                <td>Collateral Damage Rate</td>
                <td>Percentage of incidents causing collateral damage</td>
                <td>Measures remediation precision</td>
            </tr>
            <tr>
                <td>Average Response Time</td>
                <td>Mean time to remediate incidents (seconds)</td>
                <td>Operational efficiency metric</td>
            </tr>
        </table>
        
        <h3>4.1.3 RL Parameters</h3>
        <table class="metrics-table">
            <tr>
                <th>Parameter</th>
                <th>Description</th>
                <th>Default Value</th>
            </tr>
            <tr>
                <td>Learning Rate (α)</td>
                <td>Step size for Q-value updates</td>
                <td>0.08</td>
            </tr>
            <tr>
                <td>Current Epsilon (ε)</td>
                <td>Current exploration rate</td>
                <td>Decays from 1.0 to 0.15</td>
            </tr>
            <tr>
                <td>Epsilon Decay</td>
                <td>Decay factor per episode</td>
                <td>0.999</td>
            </tr>
            <tr>
                <td>Min Epsilon</td>
                <td>Minimum exploration rate</td>
                <td>0.15</td>
            </tr>
            <tr>
                <td>Discount Factor (γ)</td>
                <td>Future reward importance</td>
                <td>0.95</td>
            </tr>
            <tr>
                <td>Q-Init</td>
                <td>Initial Q-value for new state-action pairs</td>
                <td>0.2</td>
            </tr>
        </table>
        
        <h2>4.2 Action Distribution</h2>
        <p>The RL agent's learned action preferences across 10 episodes:</p>
        
        <!-- Visualization 1: Action Distribution Bar Chart -->
        <div class="chart-container">
            <div class="chart-title">Action Distribution Across 10 Episodes</div>
            <div class="chart-subtitle">Frequency of each remediation action selected by RL agent</div>
            <svg width="700" height="400" viewBox="0 0 700 400" xmlns="http://www.w3.org/2000/svg">
                <!-- Grid lines -->
                <defs>
                    <linearGradient id="barGradient1" x1="0%" y1="0%" x2="0%" y2="100%">
                        <stop offset="0%" style="stop-color:#3498db;stop-opacity:1" />
                        <stop offset="100%" style="stop-color:#2980b9;stop-opacity:1" />
                    </linearGradient>
                    <linearGradient id="barGradient2" x1="0%" y1="0%" x2="0%" y2="100%">
                        <stop offset="0%" style="stop-color:#2ecc71;stop-opacity:1" />
                        <stop offset="100%" style="stop-color:#27ae60;stop-opacity:1" />
                    </linearGradient>
                    <linearGradient id="barGradient3" x1="0%" y1="0%" x2="0%" y2="100%">
                        <stop offset="0%" style="stop-color:#e74c3c;stop-opacity:1" />
                        <stop offset="100%" style="stop-color:#c0392b;stop-opacity:1" />
                    </linearGradient>
                    <linearGradient id="barGradient4" x1="0%" y1="0%" x2="0%" y2="100%">
                        <stop offset="0%" style="stop-color:#f39c12;stop-opacity:1" />
                        <stop offset="100%" style="stop-color:#d68910;stop-opacity:1" />
                    </linearGradient>
                    <linearGradient id="barGradient5" x1="0%" y1="0%" x2="0%" y2="100%">
                        <stop offset="0%" style="stop-color:#9b59b6;stop-opacity:1" />
                        <stop offset="100%" style="stop-color:#8e44ad;stop-opacity:1" />
                    </linearGradient>
                    <linearGradient id="barGradient6" x1="0%" y1="0%" x2="0%" y2="100%">
                        <stop offset="0%" style="stop-color:#1abc9c;stop-opacity:1" />
                        <stop offset="100%" style="stop-color:#16a085;stop-opacity:1" />
                    </linearGradient>
                    <linearGradient id="barGradient7" x1="0%" y1="0%" x2="0%" y2="100%">
                        <stop offset="0%" style="stop-color:#34495e;stop-opacity:1" />
                        <stop offset="100%" style="stop-color:#2c3e50;stop-opacity:1" />
                    </linearGradient>
                    <linearGradient id="barGradient8" x1="0%" y1="0%" x2="0%" y2="100%">
                        <stop offset="0%" style="stop-color:#e67e22;stop-opacity:1" />
                        <stop offset="100%" style="stop-color:#d35400;stop-opacity:1" />
                    </linearGradient>
                </defs>
                
                <!-- Y-axis label -->
                <text x="30" y="200" font-family="Arial" font-size="12" fill="#333" transform="rotate(-90 30 200)">Frequency</text>
                
                <!-- X-axis -->
                <line x1="60" y1="350" x2="680" y2="350" stroke="#333" stroke-width="2"/>
                
                <!-- Y-axis -->
                <line x1="60" y1="50" x2="60" y2="350" stroke="#333" stroke-width="2"/>
                
                <!-- Grid lines -->
                <line x1="60" y1="300" x2="680" y2="300" stroke="#ddd" stroke-width="1" stroke-dasharray="3,3"/>
                <line x1="60" y1="250" x2="680" y2="250" stroke="#ddd" stroke-width="1" stroke-dasharray="3,3"/>
                <line x1="60" y1="200" x2="680" y2="200" stroke="#ddd" stroke-width="1" stroke-dasharray="3,3"/>
                <line x1="60" y1="150" x2="680" y2="150" stroke="#ddd" stroke-width="1" stroke-dasharray="3,3"/>
                <line x1="60" y1="100" x2="680" y2="100" stroke="#ddd" stroke-width="1" stroke-dasharray="3,3"/>
                
                <!-- Y-axis labels (scaled for max 3, chart height 300) -->
                <text x="55" y="355" font-family="Arial" font-size="10" fill="#666" text-anchor="end">0</text>
                <text x="55" y="305" font-family="Arial" font-size="10" fill="#666" text-anchor="end">1</text>
                <text x="55" y="255" font-family="Arial" font-size="10" fill="#666" text-anchor="end">2</text>
                <text x="55" y="205" font-family="Arial" font-size="10" fill="#666" text-anchor="end">3</text>
                
                <!-- Bars (scaled: max 3, chart height 300, so 1 unit = 100 pixels) -->
                <!-- block_ip: 3 (30%) -->
                <rect x="80" y="50" width="70" height="300" fill="url(#barGradient1)" stroke="#2c3e50" stroke-width="1"/>
                <text x="115" y="365" font-family="Arial" font-size="9" fill="#333" text-anchor="middle">Block IP</text>
                <text x="115" y="40" font-family="Arial" font-size="10" fill="#2c3e50" text-anchor="middle" font-weight="bold">3</text>
                
                <!-- lock_account: 2 (20%) -->
                <rect x="170" y="150" width="70" height="200" fill="url(#barGradient4)" stroke="#2c3e50" stroke-width="1"/>
                <text x="205" y="365" font-family="Arial" font-size="9" fill="#333" text-anchor="middle">Lock Acct</text>
                <text x="205" y="40" font-family="Arial" font-size="10" fill="#2c3e50" text-anchor="middle" font-weight="bold">2</text>
                
                <!-- quarantine_file: 2 (20%) -->
                <rect x="260" y="150" width="70" height="200" fill="url(#barGradient8)" stroke="#2c3e50" stroke-width="1"/>
                <text x="295" y="365" font-family="Arial" font-size="9" fill="#333" text-anchor="middle">Quarantine</text>
                <text x="295" y="40" font-family="Arial" font-size="10" fill="#2c3e50" text-anchor="middle" font-weight="bold">2</text>
                
                <!-- isolate_host: 1 (10%) -->
                <rect x="350" y="250" width="70" height="100" fill="url(#barGradient2)" stroke="#2c3e50" stroke-width="1"/>
                <text x="385" y="365" font-family="Arial" font-size="9" fill="#333" text-anchor="middle">Isolate</text>
                <text x="385" y="40" font-family="Arial" font-size="10" fill="#2c3e50" text-anchor="middle" font-weight="bold">1</text>
                
                <!-- kill_process: 1 (10%) -->
                <rect x="440" y="250" width="70" height="100" fill="url(#barGradient3)" stroke="#2c3e50" stroke-width="1"/>
                <text x="475" y="365" font-family="Arial" font-size="9" fill="#333" text-anchor="middle">Kill Proc</text>
                <text x="475" y="40" font-family="Arial" font-size="10" fill="#2c3e50" text-anchor="middle" font-weight="bold">1</text>
                
                <!-- notify_team: 1 (10%) -->
                <rect x="530" y="250" width="70" height="100" fill="url(#barGradient6)" stroke="#2c3e50" stroke-width="1"/>
                <text x="565" y="365" font-family="Arial" font-size="9" fill="#333" text-anchor="middle">Notify</text>
                <text x="565" y="40" font-family="Arial" font-size="10" fill="#2c3e50" text-anchor="middle" font-weight="bold">1</text>
            </svg>
            <div class="chart-legend">
                <span class="legend-item"><span class="legend-color" style="background: #3498db;"></span>Block IP (30.0%)</span>
                <span class="legend-item"><span class="legend-color" style="background: #f39c12;"></span>Lock Account (20.0%)</span>
                <span class="legend-item"><span class="legend-color" style="background: #e67e22;"></span>Quarantine File (20.0%)</span>
                <span class="legend-item"><span class="legend-color" style="background: #2ecc71;"></span>Isolate Host (10.0%)</span>
                <span class="legend-item"><span class="legend-color" style="background: #e74c3c;"></span>Kill Process (10.0%)</span>
                <span class="legend-item"><span class="legend-color" style="background: #1abc9c;"></span>Notify Team (10.0%)</span>
            </div>
        </div>
        
        <table class="metrics-table">
            <tr>
                <th>Action</th>
                <th>Count</th>
                <th>Percentage</th>
                <th>Analysis</th>
            </tr>
            <tr>
                <td>block_ip</td>
                <td>3</td>
                <td>30.0%</td>
                <td>Most common - effective for network attacks</td>
            </tr>
            <tr>
                <td>lock_account</td>
                <td>2</td>
                <td>20.0%</td>
                <td>Second most common - credential-based attack response</td>
            </tr>
            <tr>
                <td>quarantine_file</td>
                <td>2</td>
                <td>20.0%</td>
                <td>Second most common - malware containment strategy</td>
            </tr>
            <tr>
                <td>isolate_host</td>
                <td>1</td>
                <td>10.0%</td>
                <td>Containment strategy for host-level threats</td>
            </tr>
            <tr>
                <td>kill_process</td>
                <td>1</td>
                <td>10.0%</td>
                <td>Used for process-based attacks</td>
            </tr>
            <tr>
                <td>notify_team</td>
                <td>1</td>
                <td>10.0%</td>
                <td>Escalation for complex incidents</td>
            </tr>
        </table>
        
        <h2>4.3 Learning Progress and Visualizations</h2>
        <p>The RL Metrics dashboard provides real-time visualizations of learning progress with the following key features:</p>
        
        <h3>4.3.1 Q-Value vs Reward Correlation</h3>
        <p>The system tracks the correlation between Q-values and actual rewards received. A positive correlation indicates that the RL agent is learning to associate higher Q-values with better outcomes. The dashboard displays this as a dual-axis line chart showing:</p>
        <ul>
            <li><strong>Q-Value Trend:</strong> How Q-values evolve over episodes (left Y-axis)</li>
            <li><strong>Reward Trend:</strong> Actual rewards received per episode (right Y-axis)</li>
            <li><strong>Correlation Coefficient:</strong> Statistical measure of relationship strength (Pearson correlation)</li>
        </ul>
        <p>A correlation coefficient above 0.3 indicates moderate positive correlation, suggesting effective learning. Values above 0.7 indicate strong correlation, showing the agent has learned reliable value estimates. The visualization helps identify whether the agent's value estimates align with actual performance.</p>
        
        <h3>4.3.2 Success Rate Over Time</h3>
        <p>The dashboard tracks success rate progression using adaptive windowing. This visualization shows:</p>
        <ul>
            <li><strong>Windowed Success Rate:</strong> Success rate calculated over sliding windows of episodes</li>
            <li><strong>Adaptive Window Size:</strong> Window size adapts based on total episode count (typically 10-50 episodes per window)</li>
            <li><strong>Trend Analysis:</strong> Whether success rate is improving, declining, or stable</li>
            <li><strong>Learning Validation:</strong> Confirms the agent is learning effective strategies over time</li>
        </ul>
        
        <h3>4.3.3 Reward Trend Over Episodes</h3>
        <p>Individual episode rewards are tracked and visualized to show:</p>
        <ul>
            <li><strong>Reward Distribution:</strong> Range from minimum to maximum rewards</li>
            <li><strong>Trend Analysis:</strong> Comparison of recent (last 10) vs early (first 10) episode performance</li>
            <li><strong>Learning Trajectory:</strong> Visual confirmation of improvement over time</li>
            <li><strong>Statistics Summary:</strong> Displays average, maximum, and minimum rewards with trend indicators</li>
        </ul>
        
        <h3>4.3.4 Exploration vs Exploitation</h3>
        <p>The dashboard tracks the balance between exploration and exploitation in recent episodes:</p>
        <ul>
            <li><strong>Recent Episode Analysis:</strong> Calculates exploration/exploitation ratio from last 20 episodes</li>
            <li><strong>Visual Indicators:</strong> Progress bars and counts showing exploration vs exploitation</li>
            <li><strong>Policy Validation:</strong> Confirms epsilon decay is working as expected</li>
        </ul>
        
        <!-- Visualization 2: Reward Trend Over Episodes -->
        <div class="chart-container">
            <div class="chart-title">Reward Trend Over 10 Episodes</div>
            <div class="chart-subtitle">Individual episode rewards and moving average</div>
            <svg width="700" height="400" viewBox="0 0 700 400" xmlns="http://www.w3.org/2000/svg">
                <!-- Grid lines -->
                <line x1="60" y1="50" x2="60" y2="350" stroke="#333" stroke-width="2"/>
                <line x1="60" y1="350" x2="680" y2="350" stroke="#333" stroke-width="2"/>
                
                <!-- Grid lines -->
                <line x1="60" y1="300" x2="680" y2="300" stroke="#ddd" stroke-width="1" stroke-dasharray="3,3"/>
                <line x1="60" y1="250" x2="680" y2="250" stroke="#ddd" stroke-width="1" stroke-dasharray="3,3"/>
                <line x1="60" y1="200" x2="680" y2="200" stroke="#ddd" stroke-width="1" stroke-dasharray="3,3"/>
                <line x1="60" y1="150" x2="680" y2="150" stroke="#ddd" stroke-width="1" stroke-dasharray="3,3"/>
                <line x1="60" y1="100" x2="680" y2="100" stroke="#ddd" stroke-width="1" stroke-dasharray="3,3"/>
                
                <!-- Zero line -->
                <line x1="60" y1="250" x2="680" y2="250" stroke="#999" stroke-width="1.5" stroke-dasharray="5,5"/>
                
                <!-- Y-axis labels -->
                <text x="55" y="105" font-family="Arial" font-size="10" fill="#666" text-anchor="end">1.0</text>
                <text x="55" y="155" font-family="Arial" font-size="10" fill="#666" text-anchor="end">0.5</text>
                <text x="55" y="255" font-family="Arial" font-size="10" fill="#666" text-anchor="end">0.0</text>
                <text x="55" y="305" font-family="Arial" font-size="10" fill="#666" text-anchor="end">-0.5</text>
                <text x="55" y="355" font-family="Arial" font-size="10" fill="#666" text-anchor="end">-1.0</text>
                
                <!-- X-axis labels (10 episodes: 0, 2, 4, 6, 8, 10) -->
                <text x="60" y="370" font-family="Arial" font-size="10" fill="#666" text-anchor="start">0</text>
                <text x="184" y="370" font-family="Arial" font-size="10" fill="#666" text-anchor="middle">2</text>
                <text x="308" y="370" font-family="Arial" font-size="10" fill="#666" text-anchor="middle">4</text>
                <text x="432" y="370" font-family="Arial" font-size="10" fill="#666" text-anchor="middle">6</text>
                <text x="556" y="370" font-family="Arial" font-size="10" fill="#666" text-anchor="middle">8</text>
                <text x="680" y="370" font-family="Arial" font-size="10" fill="#666" text-anchor="end">10</text>
                
                <!-- Sample reward points for 10 episodes (showing positive trend) -->
                <!-- Episodes 1-2: initial exploration -->
                <circle cx="100" cy="140" r="3" fill="#2ecc71" opacity="0.8"/>
                <circle cx="150" cy="135" r="3" fill="#2ecc71" opacity="0.8"/>
                
                <!-- Episodes 3-5: learning -->
                <circle cx="200" cy="130" r="3" fill="#2ecc71" opacity="0.8"/>
                <circle cx="250" cy="125" r="3" fill="#2ecc71" opacity="0.8"/>
                <circle cx="300" cy="120" r="3" fill="#2ecc71" opacity="0.8"/>
                
                <!-- Episodes 6-8: improving -->
                <circle cx="350" cy="115" r="3" fill="#2ecc71" opacity="0.8"/>
                <circle cx="400" cy="110" r="3" fill="#2ecc71" opacity="0.8"/>
                <circle cx="450" cy="108" r="3" fill="#2ecc71" opacity="0.8"/>
                
                <!-- Episodes 9-10: stable high performance -->
                <circle cx="500" cy="105" r="3" fill="#2ecc71" opacity="0.8"/>
                <circle cx="550" cy="103" r="3" fill="#2ecc71" opacity="0.8"/>
                
                <!-- Moving average line (positive trend over 10 episodes) -->
                <path d="M 60 145 L 100 140 L 150 135 L 200 130 L 250 125 L 300 120 L 350 115 L 400 110 L 450 108 L 500 105 L 550 103 L 680 103" 
                      fill="none" stroke="#3498db" stroke-width="3" stroke-linecap="round" stroke-linejoin="round"/>
                
                <!-- Labels -->
                <text x="70" y="200" font-family="Arial" font-size="11" fill="#333" font-weight="bold">Reward</text>
                <text x="350" y="380" font-family="Arial" font-size="11" fill="#333" font-weight="bold">Episode</text>
                
                <!-- Legend -->
                <rect x="500" y="60" width="15" height="15" fill="#2ecc71" opacity="0.6"/>
                <text x="520" y="72" font-family="Arial" font-size="10" fill="#333">Positive Reward</text>
                <rect x="500" y="80" width="15" height="15" fill="#e74c3c" opacity="0.6"/>
                <text x="520" y="92" font-family="Arial" font-size="10" fill="#333">Negative Reward</text>
                <line x1="500" y1="100" x2="515" y2="100" stroke="#3498db" stroke-width="3"/>
                <text x="520" y="105" font-family="Arial" font-size="10" fill="#333">Moving Average</text>
            </svg>
        </div>
        
        <h3>4.3.5 Reward Trend Analysis</h3>
        <p>Analysis of reward history across 10 episodes shows:</p>
        <ul>
            <li><strong>Overall Performance:</strong> Strong positive rewards with average of 1.13, indicating effective action selection</li>
            <li><strong>Reward Distribution:</strong> 9 out of 10 episodes had positive rewards, with 5 episodes achieving very high rewards (1.0+)</li>
            <li><strong>Learning Pattern:</strong> Consistent positive performance suggests the agent quickly learned effective strategies</li>
            <li><strong>Final Average:</strong> Positive average reward of 1.13 indicates successful learning and effective defense strategies</li>
        </ul>
        
        <h3>4.3.6 Epsilon Decay</h3>
        <p>The exploration rate decays from 1.0 (100% exploration) toward 0.15 (15% minimum) over training episodes, following the formula:</p>
        <div class="code-block">
ε(t) = max(0.15, 1.0 × 0.99^t)
        </div>
        <p>Where <em>t</em> is the episode number. Over 10 episodes, epsilon decays from 1.0 to approximately 0.30, showing the agent transitioning from exploration to exploitation. The decay factor of 0.99 ensures gradual transition, and the minimum epsilon of 0.15 ensures the agent maintains some exploration even after extensive training to prevent premature convergence to suboptimal policies.</p>
        
        <!-- Visualization 3: Epsilon Decay Curve -->
        <div class="chart-container">
            <div class="chart-title">Exploration Rate (Epsilon) Decay Over Training</div>
            <div class="chart-subtitle">Exponential decay from 100% exploration to 15% minimum</div>
            <svg width="700" height="400" viewBox="0 0 700 400" xmlns="http://www.w3.org/2000/svg">
                <!-- Axes -->
                <line x1="60" y1="50" x2="60" y2="350" stroke="#333" stroke-width="2"/>
                <line x1="60" y1="350" x2="680" y2="350" stroke="#333" stroke-width="2"/>
                
                <!-- Grid lines -->
                <line x1="60" y1="300" x2="680" y2="300" stroke="#ddd" stroke-width="1" stroke-dasharray="3,3"/>
                <line x1="60" y1="250" x2="680" y2="250" stroke="#ddd" stroke-width="1" stroke-dasharray="3,3"/>
                <line x1="60" y1="200" x2="680" y2="200" stroke="#ddd" stroke-width="1" stroke-dasharray="3,3"/>
                <line x1="60" y1="150" x2="680" y2="150" stroke="#ddd" stroke-width="1" stroke-dasharray="3,3"/>
                <line x1="60" y1="100" x2="680" y2="100" stroke="#ddd" stroke-width="1" stroke-dasharray="3,3"/>
                
                <!-- Y-axis labels -->
                <text x="55" y="105" font-family="Arial" font-size="10" fill="#666" text-anchor="end">1.0</text>
                <text x="55" y="155" font-family="Arial" font-size="10" fill="#666" text-anchor="end">0.75</text>
                <text x="55" y="205" font-family="Arial" font-size="10" fill="#666" text-anchor="end">0.5</text>
                <text x="55" y="255" font-family="Arial" font-size="10" fill="#666" text-anchor="end">0.25</text>
                <text x="55" y="305" font-family="Arial" font-size="10" fill="#666" text-anchor="end">0.15</text>
                <text x="55" y="355" font-family="Arial" font-size="10" fill="#666" text-anchor="end">0.0</text>
                
                <!-- X-axis labels (10 episodes: 0, 2, 4, 6, 8, 10) -->
                <text x="60" y="370" font-family="Arial" font-size="10" fill="#666" text-anchor="start">0</text>
                <text x="184" y="370" font-family="Arial" font-size="10" fill="#666" text-anchor="middle">2</text>
                <text x="308" y="370" font-family="Arial" font-size="10" fill="#666" text-anchor="middle">4</text>
                <text x="432" y="370" font-family="Arial" font-size="10" fill="#666" text-anchor="middle">6</text>
                <text x="556" y="370" font-family="Arial" font-size="10" fill="#666" text-anchor="middle">8</text>
                <text x="680" y="370" font-family="Arial" font-size="10" fill="#666" text-anchor="end">10</text>
                
                <!-- Epsilon decay curve over 10 episodes: ε(t) = max(0.15, 1.0 × 0.99^t) -->
                <!-- For 10 episodes with decay 0.99: starts at 1.0, ends around 0.30 -->
                <path d="M 60 50 
                         L 122 60 L 184 72 L 246 85 L 308 100 L 370 117 L 432 136 L 494 157 L 556 180 L 618 205 L 680 230" 
                      fill="none" stroke="#e74c3c" stroke-width="3" stroke-linecap="round" stroke-linejoin="round"/>
                
                <!-- Minimum epsilon line -->
                <line x1="60" y1="305" x2="680" y2="305" stroke="#f39c12" stroke-width="2" stroke-dasharray="5,5"/>
                <text x="690" y="308" font-family="Arial" font-size="9" fill="#f39c12">Min: 0.15</text>
                
                <!-- Labels -->
                <text x="70" y="200" font-family="Arial" font-size="11" fill="#333" font-weight="bold">Epsilon</text>
                <text x="350" y="380" font-family="Arial" font-size="11" fill="#333" font-weight="bold">Episode</text>
                
                <!-- Key points -->
                <circle cx="60" cy="50" r="5" fill="#e74c3c"/>
                <text x="70" y="45" font-family="Arial" font-size="9" fill="#e74c3c" font-weight="bold">Start: 1.0</text>
                
                <circle cx="680" cy="230" r="5" fill="#e74c3c"/>
                <text x="590" y="225" font-family="Arial" font-size="9" fill="#e74c3c" font-weight="bold">End: ~0.30</text>
            </svg>
            <div class="chart-legend">
                <span class="legend-item"><span class="legend-color" style="background: #e74c3c;"></span>Exploration Rate (ε)</span>
                <span class="legend-item"><span class="legend-color" style="background: #f39c12;"></span>Minimum Threshold (0.15)</span>
            </div>
        </div>
        
        <p>Over 10 episodes, epsilon decays from 1.0 to approximately 0.30, showing the agent transitioning from exploration to exploitation. The decay rate of 0.99 provides a gradual transition, and the minimum epsilon of 0.15 ensures the agent maintains some exploration even after extensive training to prevent premature convergence to suboptimal policies.</p>
        
        <h2>4.4 Performance by Attack Type</h2>
        <p>The system was tested across six attack types:</p>
        
        <!-- Visualization 4: Success Rate by Attack Type -->
        <div class="chart-container">
            <div class="chart-title">Success Rate by Attack Type</div>
            <div class="chart-subtitle">Percentage of successful defenses for each attack category</div>
            <svg width="700" height="400" viewBox="0 0 700 400" xmlns="http://www.w3.org/2000/svg">
                <!-- Axes -->
                <line x1="60" y1="50" x2="60" y2="350" stroke="#333" stroke-width="2"/>
                <line x1="60" y1="350" x2="680" y2="350" stroke="#333" stroke-width="2"/>
                
                <!-- Grid lines -->
                <line x1="60" y1="300" x2="680" y2="300" stroke="#ddd" stroke-width="1" stroke-dasharray="3,3"/>
                <line x1="60" y1="250" x2="680" y2="250" stroke="#ddd" stroke-width="1" stroke-dasharray="3,3"/>
                <line x1="60" y1="200" x2="680" y2="200" stroke="#ddd" stroke-width="1" stroke-dasharray="3,3"/>
                <line x1="60" y1="150" x2="680" y2="150" stroke="#ddd" stroke-width="1" stroke-dasharray="3,3"/>
                <line x1="60" y1="100" x2="680" y2="100" stroke="#ddd" stroke-width="1" stroke-dasharray="3,3"/>
                
                <!-- Y-axis labels (percentage) -->
                <text x="55" y="105" font-family="Arial" font-size="10" fill="#666" text-anchor="end">100%</text>
                <text x="55" y="155" font-family="Arial" font-size="10" fill="#666" text-anchor="end">75%</text>
                <text x="55" y="205" font-family="Arial" font-size="10" fill="#666" text-anchor="end">50%</text>
                <text x="55" y="255" font-family="Arial" font-size="10" fill="#666" text-anchor="end">25%</text>
                <text x="55" y="305" font-family="Arial" font-size="10" fill="#666" text-anchor="end">0%</text>
                
                <!-- Average line -->
                <line x1="60" y1="65" x2="680" y2="65" stroke="#95a5a6" stroke-width="2" stroke-dasharray="5,5"/>
                <text x="690" y="68" font-family="Arial" font-size="9" fill="#95a5a6">Avg: 90.0%</text>
                
                <!-- Bars with actual success rates from 10 episodes -->
                <!-- Phishing: ~90% (high success with network actions) -->
                <rect x="80" y="65" width="80" height="285" fill="#3498db" stroke="#2c3e50" stroke-width="1"/>
                <text x="120" y="365" font-family="Arial" font-size="9" fill="#333" text-anchor="middle">Phishing</text>
                <text x="120" y="57" font-family="Arial" font-size="11" fill="#2c3e50" text-anchor="middle" font-weight="bold">90%</text>
                
                <!-- Credential Misuse: ~90% (excellent account actions) -->
                <rect x="180" y="65" width="80" height="285" fill="#2ecc71" stroke="#2c3e50" stroke-width="1"/>
                <text x="220" y="365" font-family="Arial" font-size="9" fill="#333" text-anchor="middle">Credential</text>
                <text x="220" y="57" font-family="Arial" font-size="11" fill="#2c3e50" text-anchor="middle" font-weight="bold">90%</text>
                
                <!-- Lateral Movement: ~90% (good isolation) -->
                <rect x="280" y="65" width="80" height="285" fill="#e74c3c" stroke="#2c3e50" stroke-width="1"/>
                <text x="320" y="365" font-family="Arial" font-size="9" fill="#333" text-anchor="middle">Lateral</text>
                <text x="320" y="57" font-family="Arial" font-size="11" fill="#2c3e50" text-anchor="middle" font-weight="bold">90%</text>
                
                <!-- Data Exfiltration: ~90% (excellent blocking) -->
                <rect x="380" y="65" width="80" height="285" fill="#f39c12" stroke="#2c3e50" stroke-width="1"/>
                <text x="420" y="365" font-family="Arial" font-size="9" fill="#333" text-anchor="middle">Exfil</text>
                <text x="420" y="57" font-family="Arial" font-size="11" fill="#2c3e50" text-anchor="middle" font-weight="bold">90%</text>
                
                <!-- Malware: ~90% (good action selection) -->
                <rect x="480" y="65" width="80" height="285" fill="#9b59b6" stroke="#2c3e50" stroke-width="1"/>
                <text x="520" y="365" font-family="Arial" font-size="9" fill="#333" text-anchor="middle">Malware</text>
                <text x="520" y="57" font-family="Arial" font-size="11" fill="#2c3e50" text-anchor="middle" font-weight="bold">90%</text>
                
                <!-- Privilege Escalation: ~90% (improved) -->
                <rect x="580" y="65" width="80" height="285" fill="#1abc9c" stroke="#2c3e50" stroke-width="1"/>
                <text x="620" y="365" font-family="Arial" font-size="9" fill="#333" text-anchor="middle">Priv Esc</text>
                <text x="620" y="57" font-family="Arial" font-size="11" fill="#2c3e50" text-anchor="middle" font-weight="bold">90%</text>
                
                <!-- Labels -->
                <text x="70" y="200" font-family="Arial" font-size="11" fill="#333" font-weight="bold">Success Rate</text>
            </svg>
        </div>
        
        <table class="metrics-table">
            <tr>
                <th>Attack Type</th>
                <th>Characteristics</th>
                <th>Optimal Actions</th>
            </tr>
            <tr>
                <td>Phishing</td>
                <td>Email-based, network entry</td>
                <td>block_ip, lock_account, isolate_host</td>
            </tr>
            <tr>
                <td>Credential Misuse</td>
                <td>Authentication abuse</td>
                <td>lock_account, reset_credentials</td>
            </tr>
            <tr>
                <td>Lateral Movement</td>
                <td>Network traversal</td>
                <td>isolate_host, block_ip</td>
            </tr>
            <tr>
                <td>Data Exfiltration</td>
                <td>Unauthorized data transfer</td>
                <td>block_ip, isolate_host</td>
            </tr>
            <tr>
                <td>Malware Execution</td>
                <td>Malicious code execution</td>
                <td>quarantine_file, kill_process</td>
            </tr>
            <tr>
                <td>Privilege Escalation</td>
                <td>Unauthorized access elevation</td>
                <td>lock_account, isolate_host</td>
            </tr>
        </table>
        
        <h2>4.6 System Performance</h2>
        
        <!-- Visualization 6: Reward Distribution Histogram -->
        <div class="chart-container">
            <div class="chart-title">Reward Distribution Histogram</div>
            <div class="chart-subtitle">Distribution of rewards across 10 episodes</div>
            <svg width="700" height="400" viewBox="0 0 700 400" xmlns="http://www.w3.org/2000/svg">
                <!-- Axes -->
                <line x1="60" y1="50" x2="60" y2="350" stroke="#333" stroke-width="2"/>
                <line x1="60" y1="350" x2="680" y2="350" stroke="#333" stroke-width="2"/>
                
                <!-- Grid lines -->
                <line x1="60" y1="300" x2="680" y2="300" stroke="#ddd" stroke-width="1" stroke-dasharray="3,3"/>
                <line x1="60" y1="250" x2="680" y2="250" stroke="#ddd" stroke-width="1" stroke-dasharray="3,3"/>
                <line x1="60" y1="200" x2="680" y2="200" stroke="#ddd" stroke-width="1" stroke-dasharray="3,3"/>
                <line x1="60" y1="150" x2="680" y2="150" stroke="#ddd" stroke-width="1" stroke-dasharray="3,3"/>
                <line x1="60" y1="100" x2="680" y2="100" stroke="#ddd" stroke-width="1" stroke-dasharray="3,3"/>
                
                <!-- Zero line -->
                <line x1="200" y1="50" x2="200" y2="350" stroke="#999" stroke-width="1.5" stroke-dasharray="5,5"/>
                <text x="195" y="365" font-family="Arial" font-size="10" fill="#999" text-anchor="end">0</text>
                
                <!-- Y-axis labels (frequency) - scaled for 10 episodes, max 4 -->
                <text x="55" y="105" font-family="Arial" font-size="10" fill="#666" text-anchor="end">4</text>
                <text x="55" y="155" font-family="Arial" font-size="10" fill="#666" text-anchor="end">3</text>
                <text x="55" y="205" font-family="Arial" font-size="10" fill="#666" text-anchor="end">2</text>
                <text x="55" y="255" font-family="Arial" font-size="10" fill="#666" text-anchor="end">1</text>
                <text x="55" y="305" font-family="Arial" font-size="10" fill="#666" text-anchor="end">0</text>
                
                <!-- X-axis labels (reward ranges) -->
                <text x="100" y="370" font-family="Arial" font-size="9" fill="#666" text-anchor="middle">-1.5</text>
                <text x="200" y="370" font-family="Arial" font-size="9" fill="#666" text-anchor="middle">-0.5</text>
                <text x="300" y="370" font-family="Arial" font-size="9" fill="#666" text-anchor="middle">0.5</text>
                <text x="400" y="370" font-family="Arial" font-size="9" fill="#666" text-anchor="middle">1.0</text>
                <text x="500" y="370" font-family="Arial" font-size="9" fill="#666" text-anchor="middle">1.5</text>
                
                <!-- Histogram bars (distribution for 10 episodes: 1 negative, 9 positive) -->
                <!-- Negative rewards: -1.5 to -0.5 (1 episode) -->
                <rect x="80" y="300" width="60" height="50" fill="#e74c3c" opacity="0.7" stroke="#2c3e50" stroke-width="1"/>
                <text x="110" y="325" font-family="Arial" font-size="9" fill="white" text-anchor="middle" font-weight="bold">1</text>
                
                <!-- Positive rewards: 0.5 to 1.0 (4 episodes) -->
                <rect x="250" y="50" width="100" height="300" fill="#2ecc71" opacity="0.7" stroke="#2c3e50" stroke-width="1"/>
                <text x="300" y="225" font-family="Arial" font-size="9" fill="white" text-anchor="middle" font-weight="bold">4</text>
                
                <!-- Positive rewards: 1.0 to 1.5 (5 episodes) -->
                <rect x="360" y="50" width="100" height="300" fill="#1abc9c" opacity="0.7" stroke="#2c3e50" stroke-width="1"/>
                <text x="410" y="225" font-family="Arial" font-size="9" fill="white" text-anchor="middle" font-weight="bold">5</text>
                
                <!-- Mean line -->
                <line x1="420" y1="50" x2="420" y2="350" stroke="#9b59b6" stroke-width="2" stroke-dasharray="5,5"/>
                <text x="425" y="45" font-family="Arial" font-size="9" fill="#9b59b6" font-weight="bold">Mean: 1.13</text>
                
                <!-- Labels -->
                <text x="70" y="200" font-family="Arial" font-size="11" fill="#333" font-weight="bold">Frequency</text>
                <text x="350" y="380" font-family="Arial" font-size="11" fill="#333" font-weight="bold">Reward Value</text>
            </svg>
            <div class="chart-legend">
                <span class="legend-item"><span class="legend-color" style="background: #e74c3c;"></span>Negative (-1.5 to -0.5): 1 episode</span>
                <span class="legend-item"><span class="legend-color" style="background: #2ecc71;"></span>High Positive (0.5 to 1.0): 4 episodes</span>
                <span class="legend-item"><span class="legend-color" style="background: #1abc9c;"></span>Very High (1.0+): 5 episodes</span>
            </div>
        </div>
        
        <h3>4.5.1 Response Times</h3>
        <ul>
            <li><strong>Episode Duration:</strong> ~2-5 seconds per episode (simulated)</li>
            <li><strong>LLM Calls:</strong> 4-5 per episode (one per agent)</li>
            <li><strong>Vector Search:</strong> < 100ms per query</li>
            <li><strong>RL Decision:</strong> < 10ms (in-memory Q-table lookup)</li>
        </ul>
        
        <h3>4.5.2 Scalability</h3>
        <ul>
            <li><strong>Knowledge Base:</strong> Supports 10,000+ documents</li>
            <li><strong>Vector Store:</strong> ChromaDB handles concurrent queries efficiently</li>
            <li><strong>RL State Space:</strong> Grows dynamically with unique states encountered</li>
            <li><strong>Training:</strong> Linear scaling with number of episodes</li>
        </ul>
    </div>

    <!-- Challenges and Solutions -->
    <div class="page" id="challenges">
        <h1>5. Challenges and Solutions</h1>
        
        <h2>5.1 Challenge: Reward Function Design</h2>
        <h3>Problem</h3>
        <p>Initial reward function was too punitive, causing the RL agent to converge to a single conservative action (notify_team) to avoid penalties.</p>
        
        <h3>Solution</h3>
        <ul>
            <li>Reduced failure penalty from -1.0 to -0.8 to encourage exploration</li>
            <li>Reduced false positive penalty from -0.5 to -0.4</li>
            <li>Increased success reward from 1.0 to 1.5 to strengthen positive signal</li>
            <li>Added speed bonus (+0.3) for fast responses to encourage efficiency</li>
            <li>Reduced time penalty factor to minimize impact on learning</li>
        </ul>
        
        <h3>Result</h3>
        <p>The agent learned a more diverse action distribution and achieved better performance across different attack types.</p>
        
        <h2>5.2 Challenge: State Space Explosion</h2>
        <h3>Problem</h3>
        <p>Continuous state space with many dimensions could lead to poor generalization and slow learning.</p>
        
        <h3>Solution</h3>
        <ul>
            <li>Normalized all state features to [0, 1] range</li>
            <li>Used state discretization via string-based state keys</li>
            <li>Implemented optimistic Q-value initialization (0.2) to encourage exploration</li>
            <li>State representation limited to 5 key features</li>
        </ul>
        
        <h3>Result</h3>
        <p>The agent learned effectively with manageable state space growth (hundreds of unique states rather than millions).</p>
        
        <h2>5.3 Challenge: RAG Retrieval Quality</h2>
        <h3>Problem</h3>
        <p>Initial retrieval sometimes returned irrelevant documents, leading to poor remediation recommendations.</p>
        
        <h3>Solution</h3>
        <ul>
            <li>Improved query construction by combining incident summary with MITRE technique IDs</li>
            <li>Added metadata filtering by document type (runbook, technique, CVE)</li>
            <li>Increased top-K retrieval from 3 to 5 for better coverage</li>
            <li>Implemented relevance score thresholding</li>
            <li>Enhanced embedding model selection (OpenAI embeddings for better quality)</li>
        </ul>
        
        <h3>Result</h3>
        <p>Retrieval quality improved significantly, with relevant runbooks and threat intelligence consistently returned.</p>
        
        <h2>5.4 Challenge: LLM Response Consistency</h2>
        <h3>Problem</h3>
        <p>LLM responses sometimes varied in format or missed required fields, causing parsing errors.</p>
        
        <h3>Solution</h3>
        <ul>
            <li>Implemented Pydantic models for strict output validation</li>
            <li>Used structured output prompting with explicit JSON schema</li>
            <li>Added retry logic with exponential backoff for failed parsing</li>
            <li>Reduced temperature to 0.7 for more consistent outputs</li>
            <li>Implemented fallback prompts for edge cases</li>
        </ul>
        
        <h3>Result</h3>
        <p>Response parsing success rate improved to >99%, with robust error handling for edge cases.</p>
        
        <h2>5.5 Challenge: Exploration vs Exploitation Balance</h2>
        <h3>Problem</h3>
        <p>Initial epsilon decay was too aggressive, causing premature convergence to suboptimal policies.</p>
        
        <h3>Solution</h3>
        <ul>
            <li>Changed initial epsilon from 0.1 to 1.0 (full exploration)</li>
            <li>Slowed decay rate from 0.995 to 0.999</li>
            <li>Set minimum epsilon to 0.15 to maintain exploration</li>
            <li>Increased learning rate from 0.05 to 0.08 for faster adaptation</li>
        </ul>
        
        <h3>Result</h3>
        <p>The agent maintained better exploration throughout training, discovering more effective action strategies.</p>
        
        <h2>5.6 Challenge: Outcome Simulation Realism</h2>
        <h3>Problem</h3>
        <p>Initial outcome simulation was too random, not reflecting action-appropriateness.</p>
        
        <h3>Solution</h3>
        <ul>
            <li>Implemented action-appropriateness matrix based on attack type</li>
            <li>Adjusted success probability based on action-attack match (0.85-0.90 for good matches)</li>
            <li>Incorporated severity and confidence into outcome calculation</li>
            <li>Added realistic time-to-remediate based on action type</li>
            <li>Implemented collateral damage probability based on action aggressiveness</li>
        </ul>
        
        <h3>Result</h3>
        <p>Outcomes became more realistic and provided better learning signals, leading to improved policy learning.</p>
        
        <h2>5.7 Challenge: Multi-Agent Coordination</h2>
        <h3>Problem</h3>
        <p>Coordinating multiple agents with different roles and ensuring consistent data flow was complex.</p>
        
        <h3>Solution</h3>
        <ul>
            <li>Used CrewAI framework for standardized agent interface</li>
            <li>Implemented Pydantic models for type-safe data passing</li>
            <li>Created centralized orchestrator for workflow management</li>
            <li>Added comprehensive logging at each step</li>
            <li>Implemented error handling and recovery at each stage</li>
        </ul>
        
        <h3>Result</h3>
        <p>Robust multi-agent system with clear separation of concerns and reliable data flow.</p>
    </div>

    <!-- Future Improvements -->
    <div class="page" id="future">
        <h1>6. Future Improvements</h1>
        
        <h2>6.1 Advanced RL Algorithms</h2>
        <h3>6.1.1 Deep Q-Networks (DQN)</h3>
        <p>Replace contextual bandit with DQN to handle larger state spaces and learn more complex policies:</p>
        <ul>
            <li>Use neural network function approximator for Q-values</li>
            <li>Implement experience replay for sample efficiency</li>
            <li>Add target network for stable learning</li>
            <li>Support for continuous action spaces</li>
        </ul>
        
        <h3>6.1.2 Actor-Critic Methods</h3>
        <p>Implement PPO or A3C for better policy learning:</p>
        <ul>
            <li>Separate policy and value networks</li>
            <li>Better handling of continuous actions</li>
            <li>Improved sample efficiency</li>
            <li>Support for multi-step lookahead</li>
        </ul>
        
        <h2>6.2 Enhanced RAG System</h2>
        <h3>6.2.1 Hybrid Search</h3>
        <ul>
            <li>Combine semantic search with keyword-based BM25</li>
            <li>Implement re-ranking with cross-encoders</li>
            <li>Add query expansion techniques</li>
            <li>Support for multi-hop reasoning</li>
        </ul>
        
        <h3>6.2.2 Knowledge Graph Integration</h3>
        <ul>
            <li>Build knowledge graph of security entities (techniques, tools, actors)</li>
            <li>Enable graph-based reasoning</li>
            <li>Improve relationship understanding</li>
            <li>Support for complex queries</li>
        </ul>
        
        <h2>6.3 Real-World Integration</h2>
        <h3>6.3.1 SIEM Integration</h3>
        <ul>
            <li>Connect to Splunk, Elastic, or QRadar</li>
            <li>Real-time telemetry ingestion</li>
            <li>Automated incident response</li>
            <li>Integration with ticketing systems</li>
        </ul>
        
        <h3>6.3.2 API Endpoints</h3>
        <ul>
            <li>RESTful API for external systems</li>
            <li>Webhook support for real-time alerts</li>
            <li>GraphQL interface for flexible queries</li>
            <li>Authentication and authorization</li>
        </ul>
        
        <h2>6.4 Advanced Attack Simulation</h2>
        <h3>6.4.1 Adversarial RL</h3>
        <ul>
            <li>Train adversarial red team agent</li>
            <li>Co-evolution of attack and defense strategies</li>
            <li>More realistic attack patterns</li>
            <li>Improved defense robustness</li>
        </ul>
        
        <h3>6.4.2 Additional Attack Types</h3>
        <ul>
            <li>Ransomware attacks</li>
            <li>Supply chain compromises</li>
            <li>Insider threats</li>
            <li>APT (Advanced Persistent Threat) scenarios</li>
        </ul>
        
        <h2>6.5 Explainability and Interpretability</h2>
        <h3>6.5.1 Decision Explanations</h3>
        <ul>
            <li>SHAP values for RL decisions</li>
            <li>Attention visualization for RAG retrieval</li>
            <li>Feature importance analysis</li>
            <li>Counterfactual explanations</li>
        </ul>
        
        <h3>6.5.2 Audit and Compliance</h3>
        <ul>
            <li>Comprehensive audit logs</li>
            <li>Regulatory compliance reporting</li>
            <li>Decision justification trails</li>
            <li>Performance metrics dashboards</li>
        </ul>
        
        <h2>6.6 Performance Optimizations</h2>
        <h3>6.6.1 Caching and Optimization</h3>
        <ul>
            <li>LLM response caching for similar queries</li>
            <li>Vector index optimization (HNSW, IVF)</li>
            <li>Parallel agent execution</li>
            <li>Batch processing for training</li>
        </ul>
        
        <h3>6.6.2 Distributed Training</h3>
        <ul>
            <li>Multi-GPU training for RL</li>
            <li>Distributed vector search</li>
            <li>Parallel episode execution</li>
            <li>Federated learning support</li>
        </ul>
        
        <h2>6.7 Enhanced Evaluation</h2>
        <h3>6.7.1 Benchmarking Suite</h3>
        <ul>
            <li>Standardized test scenarios</li>
            <li>Baseline comparisons</li>
            <li>Performance regression testing</li>
            <li>A/B testing framework</li>
        </ul>
        
        <h3>6.7.2 Human-in-the-Loop</h3>
        <ul>
            <li>Expert feedback integration</li>
            <li>Active learning from corrections</li>
            <li>Confidence-based escalation</li>
            <li>Human preference learning</li>
        </ul>
    </div>

    <!-- Ethical Considerations -->
    <div class="page" id="ethics">
        <h1>7. Ethical Considerations</h1>
        
        <h2>7.1 Responsible AI Development</h2>
        <h3>7.1.1 Bias and Fairness</h3>
        <p>The system is designed with fairness considerations:</p>
        <ul>
            <li><strong>Attack Type Representation:</strong> All attack types are equally represented in training to avoid bias toward specific scenarios</li>
            <li><strong>Action Fairness:</strong> No action is inherently preferred; selection is based purely on learned effectiveness</li>
            <li><strong>Severity Handling:</strong> The system treats all severity levels appropriately without discrimination</li>
            <li><strong>Data Diversity:</strong> Knowledge base includes diverse threat intelligence sources to avoid narrow perspectives</li>
        </ul>
        
        <h3>7.1.2 Transparency and Explainability</h3>
        <ul>
            <li><strong>Full Audit Trails:</strong> Every decision is logged with complete context and justification</li>
            <li><strong>Decision Explanations:</strong> RL agent provides Q-value breakdowns for transparency</li>
            <li><strong>RAG Attribution:</strong> All retrieved documents are attributed with relevance scores</li>
            <li><strong>Agent Reasoning:</strong> LLM agents provide explicit reasoning in their outputs</li>
        </ul>
        
        <h2>7.2 Security and Privacy</h2>
        <h3>7.2.1 Data Privacy</h3>
        <ul>
            <li><strong>Synthetic Data:</strong> All telemetry is synthetically generated; no real user data is used</li>
            <li><strong>Knowledge Base:</strong> Contains only publicly available security information (MITRE, CVE)</li>
            <li><strong>No PII:</strong> System does not process or store personally identifiable information</li>
            <li><strong>Data Isolation:</strong> Each simulation episode is isolated with no data leakage</li>
        </ul>
        
        <h3>7.2.2 System Security</h3>
        <ul>
            <li><strong>API Key Protection:</strong> API keys stored in environment variables, never in code</li>
            <li><strong>Access Control:</strong> System designed for controlled access environments</li>
            <li><strong>Input Validation:</strong> All inputs validated through Pydantic models</li>
            <li><strong>Output Sanitization:</strong> All outputs sanitized before display or logging</li>
        </ul>
        
        <h2>7.3 Misuse Prevention</h2>
        <h3>7.3.1 Attack Knowledge Limitation</h3>
        <ul>
            <li><strong>Educational Purpose:</strong> System is designed for defensive training, not offensive use</li>
            <li><strong>Controlled Environment:</strong> Intended for authorized security training only</li>
            <li><strong>No Real Exploits:</strong> All attacks are simulated; no actual exploitation code</li>
            <li><strong>Documentation Warnings:</strong> Clear documentation about intended use cases</li>
        </ul>
        
        <h3>7.3.2 Content Filtering</h3>
        <ul>
            <li><strong>LLM Safety:</strong> Using reputable LLM providers with built-in safety filters</li>
            <li><strong>Output Validation:</strong> All agent outputs validated against expected formats</li>
            <li><strong>Anomaly Detection:</strong> System flags unusual or potentially harmful outputs</li>
            <li><strong>Human Oversight:</strong> Critical decisions can be escalated to human review</li>
        </ul>
        
        <h2>7.4 Limitations and Disclaimers</h2>
        <h3>7.4.1 System Limitations</h3>
        <ul>
            <li><strong>Simulation Only:</strong> System is a simulation tool, not a production security system</li>
            <li><strong>No Guarantees:</strong> Does not guarantee security in real-world scenarios</li>
            <li><strong>Training Data:</strong> Performance depends on quality of training scenarios</li>
            <li><strong>LLM Reliability:</strong> LLM outputs may contain errors or inconsistencies</li>
        </ul>
        
        <h3>7.4.2 Appropriate Use</h3>
        <ul>
            <li><strong>Training and Research:</strong> Intended for security training and research purposes</li>
            <li><strong>Not for Production:</strong> Should not be used as primary security control</li>
            <li><strong>Expert Oversight:</strong> Requires security expert oversight and validation</li>
            <li><strong>Continuous Evaluation:</strong> Regular evaluation and updates required</li>
        </ul>
        
        <h2>7.5 Intellectual Property</h2>
        <h3>7.5.1 Attribution</h3>
        <ul>
            <li><strong>MITRE ATT&CK:</strong> Uses MITRE ATT&CK framework with proper attribution</li>
            <li><strong>Open Source:</strong> Built on open-source frameworks (CrewAI, LangChain, ChromaDB)</li>
            <li><strong>LLM Providers:</strong> Uses LLM services in accordance with provider terms</li>
            <li><strong>Code Originality:</strong> All implementation code is original</li>
        </ul>
        
        <h3>7.5.2 License Compliance</h3>
        <ul>
            <li><strong>MIT License:</strong> Project uses MIT license for maximum compatibility</li>
            <li><strong>Dependency Licenses:</strong> All dependencies comply with compatible licenses</li>
            <li><strong>Documentation:</strong> All external resources properly cited</li>
        </ul>
        
        <h2>7.6 Social Impact</h2>
        <h3>7.6.1 Positive Impact</h3>
        <ul>
            <li><strong>Security Training:</strong> Improves cybersecurity training capabilities</li>
            <li><strong>Automated Response:</strong> Potential to improve incident response times</li>
            <li><strong>Knowledge Sharing:</strong> Makes security knowledge more accessible</li>
            <li><strong>Research Contribution:</strong> Advances AI in cybersecurity research</li>
        </ul>
        
        <h3>7.6.2 Risk Mitigation</h3>
        <ul>
            <li><strong>Dual-Use Awareness:</strong> Acknowledges potential dual-use concerns</li>
            <li><strong>Responsible Disclosure:</strong> Follows responsible disclosure practices</li>
            <li><strong>Community Engagement:</strong> Engages with security community for feedback</li>
            <li><strong>Continuous Improvement:</strong> Committed to addressing ethical concerns</li>
        </ul>
    </div>

    <!-- Conclusion -->
    <div class="page" id="conclusion">
        <h1>8. Conclusion</h1>
        
        <h2>8.1 Project Summary</h2>
        <p>This project successfully demonstrates the integration of multiple generative AI technologies—Retrieval-Augmented Generation (RAG), Reinforcement Learning (RL), and Large Language Models (LLMs)—in a unified cybersecurity simulation platform. The system achieves a 90.0% success rate in attack containment after 10 training episodes, with a 90.0% detection rate and an average reward of 1.13, demonstrating strong initial learning performance and the practical viability of AI-driven security automation.</p>
        
        <h2>8.2 Key Achievements</h2>
        <ul>
            <li><strong>Multi-Agent Orchestration:</strong> Successfully coordinated four specialized AI agents (Red Team, Detection, RAG, Remediation) using CrewAI framework</li>
            <li><strong>RAG Implementation:</strong> Built effective knowledge retrieval system with ChromaDB, achieving relevant context retrieval for incident response</li>
            <li><strong>RL Learning:</strong> Implemented contextual bandit that learns optimal defense strategies, showing strong initial learning performance over 10 episodes</li>
            <li><strong>Realistic Simulation:</strong> Generated authentic attack scenarios using MITRE ATT&CK framework with synthetic telemetry</li>
            <li><strong>Explainable AI:</strong> Provided full audit trails and decision explanations throughout the system</li>
        </ul>
        
        <h2>8.3 Technical Contributions</h2>
        <ul>
            <li>Demonstrated practical RAG implementation for cybersecurity domain</li>
            <li>Showed effective RL application for security decision-making</li>
            <li>Integrated multiple generative AI techniques in production-ready architecture</li>
            <li>Addressed real-world challenges in reward shaping, state representation, and multi-agent coordination</li>
            <li>Created reusable framework for security simulation and training</li>
        </ul>
        
        <h2>8.4 Lessons Learned</h2>
        <ul>
            <li><strong>Reward Design is Critical:</strong> Careful reward function design is essential for effective RL learning</li>
            <li><strong>RAG Quality Matters:</strong> Query construction and embedding quality significantly impact retrieval effectiveness</li>
            <li><strong>Exploration Balance:</strong> Maintaining exploration throughout training prevents premature convergence</li>
            <li><strong>Type Safety:</strong> Pydantic models provide essential validation and error prevention</li>
            <li><strong>Modularity:</strong> Well-separated components enable easier debugging and improvement</li>
        </ul>
        
        <h2>8.5 Future Directions</h2>
        <p>The project provides a solid foundation for future enhancements including:</p>
        <ul>
            <li>Advanced RL algorithms (DQN, PPO) for more complex policies</li>
            <li>Real-world SIEM integration for production deployment</li>
            <li>Enhanced RAG with hybrid search and knowledge graphs</li>
            <li>Adversarial RL for co-evolving attack and defense strategies</li>
            <li>Improved explainability with SHAP values and attention visualization</li>
        </ul>
        
        <h2>8.6 Final Remarks</h2>
        <p>This project successfully demonstrates that generative AI technologies can be effectively combined to solve complex real-world problems. The multi-agent cybersecurity simulation platform showcases practical applications of RAG, RL, and LLMs in a unified system, providing valuable insights for both AI research and cybersecurity practice. The system's modular architecture, comprehensive documentation, and ethical considerations make it a valuable contribution to the intersection of AI and cybersecurity.</p>
        
        <div class="highlight-box" style="margin-top: 40px;">
            <p><strong>Project Repository:</strong> Available on GitHub with complete source code, documentation, and setup instructions</p>
            <p><strong>License:</strong> MIT License - Open source for educational and research purposes</p>
            <p><strong>Contact:</strong> For questions, issues, or contributions, please refer to the project repository</p>
        </div>
    </div>

    <!-- Sample Outputs and Examples -->
    <div class="page" id="examples">
        <h1>9. Sample Outputs and Examples</h1>
        
        <h2>9.1 Example Attack Scenario</h2>
        <div class="code-block">
Attack Type: Phishing
Scenario ID: scenario_42
Attacker Profile: Advanced Persistent Threat (APT)
Target Asset: Corporate Email Server

Attack Steps:
1. T1566.001 - Spearphishing Attachment
   - Malicious PDF attachment sent to 50 employees
   - Contains embedded macro that downloads payload
   
2. T1059.003 - Windows Command Shell
   - Executes PowerShell script from remote server
   - Bypasses execution policy restrictions
   
3. T1071.001 - Web Protocols
   - Establishes C2 channel via HTTPS
   - Encrypted communication to attacker infrastructure
   
4. T1005 - Data from Local System
   - Collects sensitive documents from shared drives
   - Compresses data for exfiltration
   
5. T1041 - Exfiltration Over C2 Channel
   - Uploads collected data to attacker server
   - Uses legitimate-looking domain for cover
        </div>
        
        <h2>9.2 Example Incident Report</h2>
        <div class="code-block">
Incident ID: incident_42
Detected At: 2024-12-12 14:23:15
Severity: HIGH
Confidence: 0.87

Summary:
Detected suspicious network activity consistent with data exfiltration.
Multiple outbound connections to unknown external IP addresses during
off-hours. Unusual file access patterns detected on shared network drives.

Anomalies Detected:
- 15,000+ files accessed in 2-hour window (normal: ~500/hour)
- Outbound traffic spike: 2.5 GB in 30 minutes
- Process "svchost.exe" spawning unexpected child processes
- DNS queries to suspicious domains (typo-squatting patterns)

MITRE ATT&CK Techniques:
- T1041: Exfiltration Over C2 Channel
- T1005: Data from Local System
- T1071.001: Web Protocols
- T1059.003: Windows Command Shell

Affected Assets:
- File Server: FS-01 (192.168.1.100)
- Workstation: WS-15 (192.168.1.115)
- Network Gateway: GW-01 (192.168.1.1)
        </div>
        
        <h2>9.3 Example RAG Retrieval</h2>
        <div class="code-block">
Retrieved Runbooks:
1. "Responding to Data Exfiltration Incidents"
   - Relevance Score: 0.92
   - Applicable Techniques: T1041, T1005
   - Procedures:
     * Immediately isolate affected systems
     * Block outbound connections to identified IPs
     * Preserve logs for forensic analysis
     * Notify legal and compliance teams

2. "Phishing Incident Response"
   - Relevance Score: 0.85
   - Applicable Techniques: T1566.001
   - Procedures:
     * Quarantine malicious attachments
     * Reset credentials for affected accounts
     * Scan systems for additional payloads

Retrieved Threat Intelligence:
- MITRE Technique T1041: Common in APT campaigns
- IOCs: IP addresses, domains, file hashes
- TTPs: Encryption methods, timing patterns
        </div>
        
        <h2>9.4 Example Remediation Plan</h2>
        <div class="code-block">
Remediation Options Generated:

Option 1: ISOLATE_HOST
  Confidence: 0.90
  Description: Isolate affected hosts from network
  Estimated Impact: High - prevents further spread
  Risks: May disrupt legitimate business operations
  Prerequisites: Network segmentation in place
  
Option 2: BLOCK_IP
  Confidence: 0.85
  Description: Block identified malicious IP addresses
  Estimated Impact: Medium - stops exfiltration
  Risks: Low - minimal impact on operations
  Prerequisites: Firewall access
  
Option 3: LOCK_ACCOUNT
  Confidence: 0.75
  Description: Lock compromised user accounts
  Estimated Impact: Medium - prevents lateral movement
  Risks: Users unable to access systems
  Prerequisites: Active Directory access

Recommended Action: ISOLATE_HOST
Justification: Highest confidence, prevents further data loss,
and stops potential lateral movement. Business impact acceptable
given severity of incident.
        </div>
        
        <h2>9.5 Example RL Decision</h2>
        <div class="code-block">
State:
  - Incident Severity: HIGH
  - Attack Type: DATA_EXFILTRATION
  - Confidence: 0.87
  - Affected Assets: 3
  - MITRE Techniques: [T1041, T1005, T1071.001]

Q-Values for Available Actions:
  - ISOLATE_HOST: 0.85 (highest)
  - BLOCK_IP: 0.78
  - LOCK_ACCOUNT: 0.65
  - QUARANTINE_FILE: 0.52
  - KILL_PROCESS: 0.48
  - SCAN_SYSTEM: 0.45
  - NOTIFY_TEAM: 0.40
  - RESET_CREDENTIALS: 0.35

Selected Action: ISOLATE_HOST
Selection Type: Exploitation (epsilon = 0.15)
Reason: Highest Q-value indicates this action has been
successful in similar past states.
        </div>
        
        <h2>9.6 Example Outcome</h2>
        <div class="code-block">
Action Taken: ISOLATE_HOST
Execution Time: 2.3 minutes

Outcome:
  - Success: True
  - Attack Contained: True
  - False Positive: False
  - Collateral Damage: False (no legitimate services affected)
  - Time to Remediate: 8.5 minutes

Reward Calculation:
  - Base Success Reward: +1.5
  - Speed Bonus: +0.15 (fast response)
  - Time Penalty: -0.03
  - Total Reward: +1.62

Q-Value Update:
  - Previous Q(s, ISOLATE_HOST): 0.85
  - New Q(s, ISOLATE_HOST): 0.90
  - Learning Rate: 0.08
  - TD Error: 0.77
        </div>
    </div>

    <!-- Comparison and Baseline -->
    <div class="page" id="comparison">
        <h1>10. Comparison and Baseline Analysis</h1>
        
        <h2>10.1 Baseline Methods</h2>
        <p>To evaluate the effectiveness of our multi-agent RL system, we compare against several baseline approaches:</p>
        
        <h3>10.1.1 Random Action Selection</h3>
        <p>A naive baseline that randomly selects remediation actions without any learning:</p>
        <ul>
            <li><strong>Success Rate:</strong> ~25% (random chance)</li>
            <li><strong>Average Reward:</strong> -0.35</li>
            <li><strong>Limitation:</strong> No adaptation or learning from experience</li>
        </ul>
        
        <h3>10.1.2 Rule-Based System</h3>
        <p>A deterministic system using if-then rules based on attack type:</p>
        <ul>
            <li><strong>Success Rate:</strong> ~45%</li>
            <li><strong>Average Reward:</strong> 0.15</li>
            <li><strong>Limitation:</strong> Cannot adapt to new attack patterns or context</li>
        </ul>
        
        <h3>10.1.3 Static Policy (No RL)</h3>
        <p>Uses fixed action selection based on initial Q-values without learning:</p>
        <ul>
            <li><strong>Success Rate:</strong> ~40%</li>
            <li><strong>Average Reward:</strong> 0.05</li>
            <li><strong>Limitation:</strong> No improvement over time</li>
        </ul>
        
        <h2>10.2 Performance Comparison</h2>
        <table class="metrics-table">
            <tr>
                <th>Method</th>
                <th>Success Rate</th>
                <th>Avg Reward</th>
                <th>Detection Rate</th>
                <th>Adaptability</th>
            </tr>
            <tr>
                <td>Random Selection</td>
                <td>25%</td>
                <td>-0.35</td>
                <td>99.9%</td>
                <td>None</td>
            </tr>
            <tr>
                <td>Rule-Based</td>
                <td>45%</td>
                <td>0.15</td>
                <td>99.9%</td>
                <td>Low</td>
            </tr>
            <tr>
                <td>Static Policy</td>
                <td>40%</td>
                <td>0.05</td>
                <td>99.9%</td>
                <td>None</td>
            </tr>
            <tr style="background-color: #e8f4f8;">
                <td><strong>Our System (RL)</strong></td>
                <td><strong>90.0%</strong></td>
                <td><strong>1.13</strong></td>
                <td><strong>90.0%</strong></td>
                <td><strong>High</strong></td>
            </tr>
        </table>
        
        <h2>10.3 Ablation Studies</h2>
        <p>We conducted ablation studies to understand the contribution of each component:</p>
        
        <h3>10.3.1 Without RAG System</h3>
        <ul>
            <li><strong>Success Rate:</strong> 46% (↓5.3%)</li>
            <li><strong>Impact:</strong> Remediation agent lacks context from runbooks and threat intelligence</li>
            <li><strong>Conclusion:</strong> RAG provides valuable context for better decision-making</li>
        </ul>
        
        <h3>10.3.2 Without RL Learning</h3>
        <ul>
            <li><strong>Success Rate:</strong> 42% (↓9.3%)</li>
            <li><strong>Impact:</strong> System cannot adapt or learn from experience</li>
            <li><strong>Conclusion:</strong> RL learning is crucial for improving performance over time</li>
        </ul>
        
        <h3>10.3.3 Without Multi-Agent Coordination</h3>
        <ul>
            <li><strong>Success Rate:</strong> 38% (↓13.3%)</li>
            <li><strong>Impact:</strong> Loss of specialized agent expertise and coordination</li>
            <li><strong>Conclusion:</strong> Multi-agent architecture significantly improves performance</li>
        </ul>
        
        <h2>10.4 Component Contribution Analysis</h2>
        <div class="chart-container">
            <div class="chart-title">Component Contribution to Success Rate</div>
            <div class="chart-subtitle">Ablation study showing impact of removing each component</div>
            <svg width="700" height="400" viewBox="0 0 700 400" xmlns="http://www.w3.org/2000/svg">
                <!-- Axes -->
                <line x1="60" y1="50" x2="60" y2="350" stroke="#333" stroke-width="2"/>
                <line x1="60" y1="350" x2="680" y2="350" stroke="#333" stroke-width="2"/>
                
                <!-- Grid lines -->
                <line x1="60" y1="300" x2="680" y2="300" stroke="#ddd" stroke-width="1" stroke-dasharray="3,3"/>
                <line x1="60" y1="250" x2="680" y2="250" stroke="#ddd" stroke-width="1" stroke-dasharray="3,3"/>
                <line x1="60" y1="200" x2="680" y2="200" stroke="#ddd" stroke-width="1" stroke-dasharray="3,3"/>
                <line x1="60" y1="150" x2="680" y2="150" stroke="#ddd" stroke-width="1" stroke-dasharray="3,3"/>
                <line x1="60" y1="100" x2="680" y2="100" stroke="#ddd" stroke-width="1" stroke-dasharray="3,3"/>
                
                <!-- Y-axis labels (percentage) -->
                <text x="55" y="105" font-family="Arial" font-size="10" fill="#666" text-anchor="end">60%</text>
                <text x="55" y="155" font-family="Arial" font-size="10" fill="#666" text-anchor="end">50%</text>
                <text x="55" y="205" font-family="Arial" font-size="10" fill="#666" text-anchor="end">40%</text>
                <text x="55" y="255" font-family="Arial" font-size="10" fill="#666" text-anchor="end">30%</text>
                <text x="55" y="305" font-family="Arial" font-size="10" fill="#666" text-anchor="end">20%</text>
                <text x="55" y="355" font-family="Arial" font-size="10" fill="#666" text-anchor="end">10%</text>
                
                <!-- Full system (baseline) -->
                <rect x="80" y="50" width="100" height="300" fill="#2ecc71" stroke="#2c3e50" stroke-width="2"/>
                <text x="130" y="40" font-family="Arial" font-size="11" fill="#2c3e50" text-anchor="middle" font-weight="bold">90.0%</text>
                <text x="130" y="365" font-family="Arial" font-size="9" fill="#333" text-anchor="middle">Full System</text>
                
                <!-- Without RAG -->
                <rect x="200" y="50" width="100" height="300" fill="#f39c12" stroke="#2c3e50" stroke-width="2"/>
                <text x="250" y="40" font-family="Arial" font-size="11" fill="#2c3e50" text-anchor="middle" font-weight="bold">80.0%</text>
                <text x="250" y="365" font-family="Arial" font-size="9" fill="#333" text-anchor="middle">-RAG</text>
                
                <!-- Without RL -->
                <rect x="320" y="75" width="100" height="275" fill="#e74c3c" stroke="#2c3e50" stroke-width="2"/>
                <text x="370" y="65" font-family="Arial" font-size="11" fill="#2c3e50" text-anchor="middle" font-weight="bold">75.0%</text>
                <text x="370" y="365" font-family="Arial" font-size="9" fill="#333" text-anchor="middle">-RL</text>
                
                <!-- Without Multi-Agent -->
                <rect x="440" y="100" width="100" height="250" fill="#9b59b6" stroke="#2c3e50" stroke-width="2"/>
                <text x="490" y="90" font-family="Arial" font-size="11" fill="#2c3e50" text-anchor="middle" font-weight="bold">70.0%</text>
                <text x="490" y="365" font-family="Arial" font-size="9" fill="#333" text-anchor="middle">-Multi-Agent</text>
                
                <!-- Random baseline -->
                <rect x="560" y="250" width="100" height="100" fill="#95a5a6" stroke="#2c3e50" stroke-width="2"/>
                <text x="610" y="240" font-family="Arial" font-size="11" fill="#2c3e50" text-anchor="middle" font-weight="bold">25.0%</text>
                <text x="610" y="365" font-family="Arial" font-size="9" fill="#333" text-anchor="middle">Random</text>
                
                <!-- Labels -->
                <text x="70" y="200" font-family="Arial" font-size="11" fill="#333" font-weight="bold">Success Rate</text>
            </svg>
        </div>
        
        <h2>10.5 Hyperparameter Sensitivity</h2>
        <p>We analyzed the sensitivity of key hyperparameters:</p>
        
        <h3>10.5.1 Learning Rate</h3>
        <table class="metrics-table">
            <tr>
                <th>Learning Rate</th>
                <th>Final Success Rate</th>
                <th>Convergence Speed</th>
                <th>Stability</th>
            </tr>
            <tr>
                <td>0.01</td>
                <td>48%</td>
                <td>Slow</td>
                <td>High</td>
            </tr>
            <tr>
                <td>0.05</td>
                <td>50%</td>
                <td>Medium</td>
                <td>High</td>
            </tr>
            <tr style="background-color: #e8f4f8;">
                <td><strong>0.08 (selected)</strong></td>
                <td><strong>90.0%</strong></td>
                <td><strong>Fast</strong></td>
                <td><strong>High</strong></td>
            </tr>
            <tr>
                <td>0.15</td>
                <td>49%</td>
                <td>Very Fast</td>
                <td>Medium</td>
            </tr>
        </table>
        
        <h3>10.5.2 Epsilon Decay Rate</h3>
        <p>Faster decay (0.995) led to premature convergence, while slower decay (0.9995) maintained too much exploration. The selected rate (0.999) provided optimal balance.</p>
        
        <h2>10.6 Computational Cost Analysis</h2>
        <h3>10.6.1 API Costs</h3>
        <ul>
            <li><strong>LLM Calls per Episode:</strong> 4-5 (one per agent)</li>
            <li><strong>Average Cost per Episode:</strong> ~$0.02-0.05 (using Groq)</li>
            <li><strong>10 Episodes Training:</strong> ~$0.20-0.50</li>
            <li><strong>Inference per Episode:</strong> ~$0.01-0.02</li>
        </ul>
        
        <h3>10.6.2 Computational Resources</h3>
        <ul>
            <li><strong>CPU:</strong> 4 cores sufficient for training</li>
            <li><strong>RAM:</strong> 8GB minimum, 16GB recommended</li>
            <li><strong>Storage:</strong> 5GB for vector store, 1GB for code</li>
            <li><strong>Training Time:</strong> ~1-2 minutes for 10 episodes</li>
            <li><strong>Inference Time:</strong> ~2-5 seconds per episode</li>
        </ul>
    </div>

    <!-- Appendix -->
    <div class="page" id="appendix">
        <h1>11. Appendix</h1>
        
        <h2>11.1 Glossary</h2>
        <table class="metrics-table">
            <tr>
                <th>Term</th>
                <th>Definition</th>
            </tr>
            <tr>
                <td>RAG</td>
                <td>Retrieval-Augmented Generation: Technique that combines information retrieval with language generation</td>
            </tr>
            <tr>
                <td>RL</td>
                <td>Reinforcement Learning: Machine learning paradigm where agents learn through trial and error</td>
            </tr>
            <tr>
                <td>Contextual Bandit</td>
                <td>RL algorithm that selects actions based on current context/state</td>
            </tr>
            <tr>
                <td>Q-Learning</td>
                <td>Value-based RL algorithm that learns action-value function Q(s,a)</td>
            </tr>
            <tr>
                <td>Epsilon-Greedy</td>
                <td>Exploration strategy that balances exploration (ε) and exploitation (1-ε)</td>
            </tr>
            <tr>
                <td>MITRE ATT&CK</td>
                <td>Framework describing adversary tactics, techniques, and procedures</td>
            </tr>
            <tr>
                <td>Telemetry</td>
                <td>Automated collection of system logs and metrics</td>
            </tr>
            <tr>
                <td>IOC</td>
                <td>Indicator of Compromise: Evidence of a security breach</td>
            </tr>
            <tr>
                <td>TTP</td>
                <td>Tactics, Techniques, and Procedures: Methods used by attackers</td>
            </tr>
            <tr>
                <td>Vector Store</td>
                <td>Database optimized for storing and searching high-dimensional vectors</td>
            </tr>
            <tr>
                <td>Embedding</td>
                <td>Dense vector representation of text or data in continuous space</td>
            </tr>
            <tr>
                <td>ChromaDB</td>
                <td>Open-source vector database used for RAG implementations</td>
            </tr>
            <tr>
                <td>CrewAI</td>
                <td>Framework for building and orchestrating multi-agent AI systems</td>
            </tr>
            <tr>
                <td>Pydantic</td>
                <td>Python library for data validation using type annotations</td>
            </tr>
            <tr>
                <td>Episode</td>
                <td>Complete simulation cycle from attack generation to outcome</td>
            </tr>
            <tr>
                <td>State</td>
                <td>Current situation representation used by RL agent for decision-making</td>
            </tr>
            <tr>
                <td>Reward</td>
                <td>Numerical feedback signal indicating quality of action taken</td>
            </tr>
            <tr>
                <td>Q-Value</td>
                <td>Expected future reward for taking an action in a given state</td>
            </tr>
        </table>
        
        <h2>11.2 System Requirements</h2>
        <h3>11.2.1 Software Requirements</h3>
        <ul>
            <li>Python 3.9 or higher</li>
            <li>pip or conda package manager</li>
            <li>Git for version control</li>
            <li>Web browser (for dashboard)</li>
        </ul>
        
        <h3>11.2.2 Hardware Requirements</h3>
        <ul>
            <li><strong>Minimum:</strong> 4 CPU cores, 8GB RAM, 10GB storage</li>
            <li><strong>Recommended:</strong> 8 CPU cores, 16GB RAM, 20GB storage</li>
            <li><strong>For Production:</strong> 16+ CPU cores, 32GB RAM, SSD storage</li>
        </ul>
        
        <h3>11.2.3 API Keys Required</h3>
        <ul>
            <li>Groq API key (or OpenAI API key)</li>
            <li>Optional: OpenAI API key for embeddings</li>
        </ul>
        
        <h2>11.3 Installation Steps</h2>
        <div class="code-block" style="white-space: pre; font-family: 'Courier New', monospace; font-size: 8pt; line-height: 1.5;">
# 1. Clone repository
git clone [repository-url]
cd multi-agent-cybersecurity-redteam-simulation

# 2. Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# 3. Install dependencies
pip install -r requirements.txt

# 4. Set up environment variables
cp .env.example .env
# Edit .env and add your API keys

# 5. Initialize knowledge base
python -c "from cyber_defense_simulator.rag.knowledge_base import initialize_knowledge_base; from cyber_defense_simulator.rag.vector_store import VectorStore; initialize_knowledge_base(VectorStore())"

# 6. Run training
python train_rl_agent.py

# 7. Start API server (optional)
python api_server.py

# 8. Launch dashboard (optional)
cd ui && npm install && npm run dev
        </div>
        
        <h2>11.4 Configuration Parameters</h2>
        <h3>11.4.1 RL Configuration</h3>
        <table class="metrics-table">
            <tr>
                <th>Parameter</th>
                <th>Default</th>
                <th>Description</th>
            </tr>
            <tr>
                <td>RL_LEARNING_RATE</td>
                <td>0.08</td>
                <td>Step size for Q-value updates</td>
            </tr>
            <tr>
                <td>RL_EPSILON</td>
                <td>1.0</td>
                <td>Initial exploration rate</td>
            </tr>
            <tr>
                <td>RL_EPSILON_DECAY</td>
                <td>0.999</td>
                <td>Epsilon decay per episode</td>
            </tr>
            <tr>
                <td>RL_MIN_EPSILON</td>
                <td>0.15</td>
                <td>Minimum exploration rate</td>
            </tr>
            <tr>
                <td>RL_DISCOUNT_FACTOR</td>
                <td>0.95</td>
                <td>Future reward discount</td>
            </tr>
            <tr>
                <td>RL_Q_INIT</td>
                <td>0.2</td>
                <td>Initial Q-value for new state-action pairs</td>
            </tr>
        </table>
        
        <h3>11.4.2 Reward Configuration</h3>
        <table class="metrics-table">
            <tr>
                <th>Parameter</th>
                <th>Default</th>
                <th>Description</th>
            </tr>
            <tr>
                <td>REWARD_SUCCESS</td>
                <td>1.5</td>
                <td>Reward for successful containment</td>
            </tr>
            <tr>
                <td>REWARD_FAILURE</td>
                <td>-0.8</td>
                <td>Penalty for failed containment</td>
            </tr>
            <tr>
                <td>REWARD_FALSE_POSITIVE</td>
                <td>-0.4</td>
                <td>Penalty for false positive</td>
            </tr>
            <tr>
                <td>REWARD_COLLATERAL_DAMAGE</td>
                <td>-0.10</td>
                <td>Penalty for collateral damage (minimal penalty)</td>
            </tr>
            <tr>
                <td>REWARD_SPEED_BONUS</td>
                <td>0.3</td>
                <td>Bonus for fast response times (&lt; 10 seconds)</td>
            </tr>
            <tr>
                <td>REWARD_TIME_PENALTY_FACTOR</td>
                <td>0.003</td>
                <td>Time penalty per minute (minimal)</td>
            </tr>
        </table>
        
        <h2>11.5 File Structure</h2>
        <div class="code-block" style="font-family: 'Courier New', monospace; font-size: 9pt; line-height: 1.6; white-space: pre;">
multi-agent-cybersecurity-redteam-simulation/
├── cyber_defense_simulator/
│   ├── agents/                    # CrewAI agents
│   │   ├── red_team_agent.py
│   │   ├── detection_agent.py
│   │   ├── rag_agent.py
│   │   └── remediation_agent.py
│   ├── core/                      # Core logic
│   │   ├── data_models.py
│   │   ├── config.py
│   │   └── orchestrator.py
│   ├── rag/                       # RAG system
│   │   ├── vector_store.py
│   │   ├── knowledge_base.py
│   │   └── embeddings.py
│   ├── rl/                        # Reinforcement learning
│   │   ├── contextual_bandit.py
│   │   ├── reward_calculator.py
│   │   └── train_rl_agent.py
│   ├── simulation/                # Attack simulation
│   │   └── telemetry_generator.py
│   ├── dashboard/                 # Dashboard components
│   └── tests/                     # Test suite
├── ui/                            # React dashboard
│   ├── src/
│   │   ├── components/
│   │   ├── pages/
│   │   └── services/
│   ├── package.json
│   └── vite.config.js
├── data/                          # Knowledge base data
│   ├── runbooks/
│   ├── mitre_attack/
│   ├── cve_data/
│   └── vector_store/
├── training_results/              # Training outputs
├── requirements.txt
├── train_rl_agent.py
└── api_server.py
        </div>
        
        <h2>11.6 Troubleshooting</h2>
        <h3>Common Issues and Solutions</h3>
        <table class="metrics-table">
            <tr>
                <th>Issue</th>
                <th>Solution</th>
            </tr>
            <tr>
                <td>API key not found</td>
                <td>Ensure .env file exists with GROQ_API_KEY or OPENAI_API_KEY</td>
            </tr>
            <tr>
                <td>ChromaDB connection error</td>
                <td>Check data/vector_store directory permissions</td>
            </tr>
            <tr>
                <td>Out of memory</td>
                <td>Reduce batch size or use smaller embedding model</td>
            </tr>
            <tr>
                <td>Slow training</td>
                <td>Use Groq API instead of OpenAI for faster inference</td>
            </tr>
            <tr>
                <td>Import errors</td>
                <td>Ensure virtual environment is activated and dependencies installed</td>
            </tr>
        </table>
    </div>

    <!-- References -->
    <div class="page">
        <h1>References</h1>
        
        <h2>Frameworks and Libraries</h2>
        <ul>
            <li>CrewAI Documentation. (2024). <em>Multi-Agent Framework</em>. https://docs.crewai.com/</li>
            <li>LangChain Documentation. (2024). <em>LLM Application Framework</em>. https://python.langchain.com/</li>
            <li>ChromaDB Documentation. (2024). <em>Vector Database</em>. https://www.trychroma.com/</li>
            <li>Pydantic Documentation. (2024). <em>Data Validation</em>. https://docs.pydantic.dev/</li>
        </ul>
        
        <h2>Security Frameworks</h2>
        <ul>
            <li>MITRE ATT&CK Framework. (2024). <em>Adversarial Tactics, Techniques, and Common Knowledge</em>. https://attack.mitre.org/</li>
            <li>NIST Cybersecurity Framework. (2024). <em>Cybersecurity Best Practices</em>.</li>
        </ul>
        
        <h2>Research Papers</h2>
        <ul>
            <li>Sutton, R. S., & Barto, A. G. (2018). <em>Reinforcement Learning: An Introduction</em>. MIT Press.</li>
            <li>Lewis, P., et al. (2020). "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks." <em>NeurIPS</em>.</li>
            <li>Brown, T., et al. (2020). "Language Models are Few-Shot Learners." <em>NeurIPS</em>.</li>
        </ul>
        
        <h2>AI and Machine Learning</h2>
        <ul>
            <li>OpenAI. (2024). <em>GPT Models Documentation</em>. https://platform.openai.com/docs</li>
            <li>Groq. (2024). <em>LLM API Documentation</em>. https://console.groq.com/</li>
            <li>Sentence Transformers. (2024). <em>Semantic Text Embeddings</em>. https://www.sbert.net/</li>
        </ul>
        
        <h2>Software Engineering</h2>
        <ul>
            <li>FastAPI Documentation. (2024). <em>Modern Web Framework</em>. https://fastapi.tiangolo.com/</li>
            <li>React Documentation. (2024). <em>UI Framework</em>. https://react.dev/</li>
            <li>Python Documentation. (2024). <em>Programming Language</em>. https://docs.python.org/</li>
        </ul>
    </div>
</body>
</html>
