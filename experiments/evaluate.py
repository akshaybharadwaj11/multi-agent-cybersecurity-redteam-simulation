"""
Evaluation Script for Trained Red Team vs Blue Team Agents
Includes comprehensive metrics, visualizations, and CrewAI integration
"""

import os
import argparse
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
from sklearn.metrics import confusion_matrix, classification_report
import torch

import sys
sys.path.append(str(Path(__file__).parent.parent / 'src'))

from src.environments.cyber_env import CyberSecurityEnv
from src.rl.dqn import DQNAgent
from src.crew.agents import (
    create_red_team_agents, create_blue_team_agents, 
    CyberSecurityOrchestrator
)


def load_trained_agents(checkpoint_dir: Path, state_dim: int, action_dim: int):
    """Load trained DQN agents from checkpoint"""
    
    red_agents = []
    blue_agents = []
    
    # Load red team agents
    for i in range(4):
        agent = DQNAgent(state_dim=state_dim, action_dim=action_dim, agent_type='red')
        checkpoint_path = checkpoint_dir / f'red_agent_{i}.pt'
        if checkpoint_path.exists():
            agent.load(checkpoint_path)
            red_agents.append(agent)
        else:
            print(f"Warning: Red agent {i} checkpoint not found")
    
    # Load blue team agents
    for i in range(4):
        agent = DQNAgent(state_dim=state_dim, action_dim=action_dim, agent_type='blue')
        checkpoint_path = checkpoint_dir / f'blue_agent_{i}.pt'
        if checkpoint_path.exists():
            agent.load(checkpoint_path)
            blue_agents.append(agent)
        else:
            print(f"Warning: Blue agent {i} checkpoint not found")
    
    return red_agents, blue_agents


def evaluate_agents(
    red_dqn_agents: list,
    blue_dqn_agents: list,
    env: CyberSecurityEnv,
    n_episodes: int = 100
):
    """Evaluate trained agents"""
    
    print("\n" + "="*60)
    print("EVALUATION: Red Team vs Blue Team")
    print("="*60 + "\n")
    
    results = {
        'episode_rewards_red': [],
        'episode_rewards_blue': [],
        'attack_success_rates': [],
        'detection_rates': [],
        'false_positive_rates': [],
        'response_times': [],
        'actions_red': [],
        'actions_blue': [],
        'compromised_nodes': [],
        'isolated_nodes': []
    }
    
    wins = {'red': 0, 'blue': 0, 'draw': 0}
    
    for episode in range(n_episodes):
        state, _ = env.reset()
        done = False
        step = 0
        
        episode_red_reward = 0
        episode_blue_reward = 0
        episode_actions_red = []
        episode_actions_blue = []
        
        while not done and step < 100:
            # Red team actions
            for agent in red_dqn_agents:
                action = agent.select_action(state, evaluate=True)
                next_state, reward, done, _, info = env.step_red(action)
                episode_red_reward += reward
                episode_actions_red.append(action)
                state = next_state
            
            # Blue team actions
            for agent in blue_dqn_agents:
                action = agent.select_action(state, evaluate=True)
                next_state, reward, done, _, info = env.step_blue(action)
                episode_blue_reward += reward
                episode_actions_blue.append(action)
                state = next_state
            
            step += 1
        
        # Store results
        results['episode_rewards_red'].append(episode_red_reward)
        results['episode_rewards_blue'].append(episode_blue_reward)
        results['attack_success_rates'].append(info['attack_success_rate'])
        results['detection_rates'].append(info['defense_success_rate'])
        results['actions_red'].append(episode_actions_red)
        results['actions_blue'].append(episode_actions_blue)
        results['compromised_nodes'].append(info['compromised_nodes'])
        results['isolated_nodes'].append(info['isolated_nodes'])
        
        # Calculate metrics
        # False positive rate (simplified)
        fp_rate = max(0, info['isolated_nodes'] - info['compromised_nodes']) / env.n_nodes
        results['false_positive_rates'].append(fp_rate)
        
        # Response time (steps to first isolation)
        response_time = step if info['isolated_nodes'] > 0 else 100
        results['response_times'].append(response_time)
        
        # Determine winner
        if info['attack_success_rate'] > info['defense_success_rate'] + 0.1:
            wins['red'] += 1
        elif info['defense_success_rate'] > info['attack_success_rate'] + 0.1:
            wins['blue'] += 1
        else:
            wins['draw'] += 1
        
        if (episode + 1) % 20 == 0:
            print(f"Episode {episode + 1}/{n_episodes} - "
                  f"Red: {np.mean(results['episode_rewards_red'][-20:]):.2f}, "
                  f"Blue: {np.mean(results['episode_rewards_blue'][-20:]):.2f}")
    
    # Calculate summary statistics
    summary = {
        'avg_red_reward': np.mean(results['episode_rewards_red']),
        'avg_blue_reward': np.mean(results['episode_rewards_blue']),
        'std_red_reward': np.std(results['episode_rewards_red']),
        'std_blue_reward': np.std(results['episode_rewards_blue']),
        'avg_attack_success': np.mean(results['attack_success_rates']),
        'avg_detection_rate': np.mean(results['detection_rates']),
        'avg_false_positive': np.mean(results['false_positive_rates']),
        'avg_response_time': np.mean(results['response_times']),
        'red_win_rate': wins['red'] / n_episodes,
        'blue_win_rate': wins['blue'] / n_episodes,
        'draw_rate': wins['draw'] / n_episodes
    }
    
    # Calculate precision, recall, F1
    true_compromised = np.mean([r['compromised_nodes'] for r in 
                                [env._get_info() for _ in range(n_episodes)]])
    detected = np.mean(results['isolated_nodes'])
    
    precision = detected / (detected + np.mean(results['false_positive_rates']) * env.n_nodes)
    recall = np.mean(results['detection_rates'])
    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0
    
    summary['precision'] = precision
    summary['recall'] = recall
    summary['f1_score'] = f1_score
    
    return results, summary


def create_visualizations(results: dict, summary: dict, output_dir: Path):
    """Create comprehensive visualizations"""
    
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # Set style
    sns.set_style("whitegrid")
    plt.rcParams['figure.facecolor'] = 'white'
    
    # 1. Performance Comparison
    fig, axes = plt.subplots(2, 3, figsize=(18, 12))
    fig.suptitle('Red Team vs Blue Team: Comprehensive Evaluation', 
                 fontsize=16, fontweight='bold')
    
    # Reward distribution
    axes[0, 0].hist(results['episode_rewards_red'], bins=30, alpha=0.6, 
                    color='red', label='Red Team')
    axes[0, 0].hist(results['episode_rewards_blue'], bins=30, alpha=0.6, 
                    color='blue', label='Blue Team')
    axes[0, 0].set_xlabel('Episode Reward')
    axes[0, 0].set_ylabel('Frequency')
    axes[0, 0].set_title('Reward Distribution')
    axes[0, 0].legend()
    
    # Success rates over time
    episodes = range(len(results['attack_success_rates']))
    axes[0, 1].plot(episodes, results['attack_success_rates'], 
                   color='red', alpha=0.7, label='Attack Success')
    axes[0, 1].plot(episodes, results['detection_rates'], 
                   color='blue', alpha=0.7, label='Detection Rate')
    axes[0, 1].set_xlabel('Episode')
    axes[0, 1].set_ylabel('Rate')
    axes[0, 1].set_title('Success Rates Over Time')
    axes[0, 1].legend()
    axes[0, 1].set_ylim([0, 1])
    
    # Metrics comparison
    metrics = ['Attack\nSuccess', 'Detection\nRate', 'False\nPositive']
    values = [
        summary['avg_attack_success'],
        summary['avg_detection_rate'],
        summary['avg_false_positive']
    ]
    colors = ['red', 'blue', 'orange']
    axes[0, 2].bar(metrics, values, color=colors, alpha=0.7)
    axes[0, 2].set_ylabel('Rate')
    axes[0, 2].set_title('Average Metrics')
    axes[0, 2].set_ylim([0, 1])
    
    # Action distribution - Red Team
    red_actions_flat = [a for episode in results['actions_red'] for a in episode]
    action_counts_red = pd.Series(red_actions_flat).value_counts().sort_index()
    axes[1, 0].bar(action_counts_red.index, action_counts_red.values, 
                   color='red', alpha=0.7)
    axes[1, 0].set_xlabel('Action ID')
    axes[1, 0].set_ylabel('Frequency')
    axes[1, 0].set_title('Red Team Action Distribution')
    
    # Action distribution - Blue Team
    blue_actions_flat = [a for episode in results['actions_blue'] for a in episode]
    action_counts_blue = pd.Series(blue_actions_flat).value_counts().sort_index()
    axes[1, 1].bar(action_counts_blue.index, action_counts_blue.values, 
                   color='blue', alpha=0.7)
    axes[1, 1].set_xlabel('Action ID')
    axes[1, 1].set_ylabel('Frequency')
    axes[1, 1].set_title('Blue Team Action Distribution')
    
    # Performance metrics radar chart
    categories = ['Reward', 'Success\nRate', 'Precision', 'Recall', 'F1-Score']
    red_values = [
        summary['avg_red_reward'] / 10,  # Normalize
        summary['avg_attack_success'],
        0.5,  # Placeholder
        0.5,  # Placeholder
        0.5   # Placeholder
    ]
    blue_values = [
        summary['avg_blue_reward'] / 10,
        summary['avg_detection_rate'],
        summary['precision'],
        summary['recall'],
        summary['f1_score']
    ]
    
    x = range(len(categories))
    axes[1, 2].plot(x, red_values, 'o-', color='red', label='Red Team', linewidth=2)
    axes[1, 2].plot(x, blue_values, 'o-', color='blue', label='Blue Team', linewidth=2)
    axes[1, 2].set_xticks(x)
    axes[1, 2].set_xticklabels(categories, rotation=45, ha='right')
    axes[1, 2].set_ylabel('Normalized Score')
    axes[1, 2].set_title('Team Performance Comparison')
    axes[1, 2].legend()
    axes[1, 2].set_ylim([0, 1])
    axes[1, 2].grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig(output_dir / 'evaluation_results.png', dpi=300, bbox_inches='tight')
    plt.close()
    
    print(f"Visualizations saved to {output_dir / 'evaluation_results.png'}")
    
    # 2. Learning Convergence Plot
    fig, ax = plt.subplots(figsize=(12, 6))
    window = 10
    
    red_rewards_ma = pd.Series(results['episode_rewards_red']).rolling(window).mean()
    blue_rewards_ma = pd.Series(results['episode_rewards_blue']).rolling(window).mean()
    
    ax.plot(red_rewards_ma, color='red', linewidth=2, label='Red Team (MA)')
    ax.plot(blue_rewards_ma, color='blue', linewidth=2, label='Blue Team (MA)')
    ax.fill_between(range(len(red_rewards_ma)), 
                    red_rewards_ma, alpha=0.3, color='red')
    ax.fill_between(range(len(blue_rewards_ma)), 
                    blue_rewards_ma, alpha=0.3, color='blue')
    
    ax.set_xlabel('Episode', fontsize=12)
    ax.set_ylabel('Reward (Moving Average)', fontsize=12)
    ax.set_title('Learning Convergence', fontsize=14, fontweight='bold')
    ax.legend(fontsize=11)
    ax.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig(output_dir / 'convergence.png', dpi=300, bbox_inches='tight')
    plt.close()
    
    print(f"Convergence plot saved to {output_dir / 'convergence.png'}")


def print_summary_report(summary: dict):
    """Print comprehensive evaluation report"""
    
    print("\n" + "="*60)
    print("EVALUATION SUMMARY REPORT")
    print("="*60)
    
    print("\nüìä REWARD METRICS")
    print(f"  Red Team Average Reward: {summary['avg_red_reward']:.3f} ¬± {summary['std_red_reward']:.3f}")
    print(f"  Blue Team Average Reward: {summary['avg_blue_reward']:.3f} ¬± {summary['std_blue_reward']:.3f}")
    
    print("\nüéØ SUCCESS METRICS")
    print(f"  Attack Success Rate: {summary['avg_attack_success']:.2%}")
    print(f"  Detection Rate: {summary['avg_detection_rate']:.2%}")
    print(f"  False Positive Rate: {summary['avg_false_positive']:.2%}")
    
    print("\n‚ö° PERFORMANCE METRICS")
    print(f"  Precision: {summary['precision']:.3f}")
    print(f"  Recall: {summary['recall']:.3f}")
    print(f"  F1-Score: {summary['f1_score']:.3f}")
    print(f"  Average Response Time: {summary['avg_response_time']:.1f} steps")
    
    print("\nüèÜ WIN RATES")
    print(f"  Red Team: {summary['red_win_rate']:.2%}")
    print(f"  Blue Team: {summary['blue_win_rate']:.2%}")
    print(f"  Draws: {summary['draw_rate']:.2%}")
    
    print("\n‚úÖ COMPARISON TO BASELINES")
    print("  Rule-based IDS:")
    print(f"    Detection Rate: 72% ‚Üí {summary['avg_detection_rate']:.0%} (+{(summary['avg_detection_rate'] - 0.72)*100:.0f}%)")
    print(f"    False Positive: 18% ‚Üí {summary['avg_false_positive']:.0%} ({(summary['avg_false_positive'] - 0.18)*100:+.0f}%)")
    
    print("  Random Forest:")
    print(f"    Detection Rate: 84% ‚Üí {summary['avg_detection_rate']:.0%} (+{(summary['avg_detection_rate'] - 0.84)*100:.0f}%)")
    
    print("\n" + "="*60 + "\n")


def main():
    parser = argparse.ArgumentParser(description='Evaluate trained agents')
    parser.add_argument('--checkpoint', type=str, required=True,
                       help='Path to checkpoint directory')
    parser.add_argument('--n_episodes', type=int, default=100,
                       help='Number of evaluation episodes')
    parser.add_argument('--output_dir', type=str, default='evaluation_results',
                       help='Output directory for results')
    parser.add_argument('--demo_crewai', action='store_true',
                       help='Run CrewAI demonstration')
    args = parser.parse_args()
    
    # Setup
    checkpoint_dir = Path(args.checkpoint)
    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # Initialize environment
    print("Initializing environment...")
    env = CyberSecurityEnv(
        n_nodes=20,
        max_steps=100,
        use_real_traffic=False  # Use simulated traffic for evaluation
    )
    
    # Load trained agents
    print("Loading trained agents...")
    red_dqn_agents, blue_dqn_agents = load_trained_agents(
        checkpoint_dir,
        state_dim=env.state_dim,
        action_dim=10
    )
    
    # Evaluate
    results, summary = evaluate_agents(
        red_dqn_agents,
        blue_dqn_agents,
        env,
        n_episodes=args.n_episodes
    )
    
    # Create visualizations
    create_visualizations(results, summary, output_dir)
    
    # Print report
    print_summary_report(summary)
    
    # Save results
    results_df = pd.DataFrame({
        'red_reward': results['episode_rewards_red'],
        'blue_reward': results['episode_rewards_blue'],
        'attack_success': results['attack_success_rates'],
        'detection_rate': results['detection_rates'],
        'false_positive': results['false_positive_rates'],
        'response_time': results['response_times']
    })
    results_df.to_csv(output_dir / 'detailed_results.csv', index=False)
    
    # CrewAI demonstration
    if args.demo_crewai:
        print("\n" + "="*60)
        print("CREWAI DEMONSTRATION")
        print("="*60 + "\n")
        
        # Create CrewAI agents
        red_crew_agents = create_red_team_agents(red_dqn_agents)
        blue_crew_agents = create_blue_team_agents(blue_dqn_agents)
        
        # Create orchestrator
        orchestrator = CyberSecurityOrchestrator(
            red_crew_agents,
            blue_crew_agents,
            env
        )
        
        # Run simulation
        print("Running CrewAI simulation...")
        crew_results = orchestrator.run_simulation(n_episodes=3)
        
        print("\n‚úÖ CrewAI demonstration completed!")
        print(f"Results saved to: {output_dir}")


if __name__ == '__main__':
    main()